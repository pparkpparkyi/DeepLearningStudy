# ëª©ì°¨
* [2. í¼ì…‰íŠ¸ë¡ ](#2-í¼ì…‰íŠ¸ë¡ )

    * [2.3.2. ê°€ì¤‘ì¹˜ í¸í–¥](#232-ê°€ì¤‘ì¹˜-í¸í–¥)

---
# 2. í¼ì…‰íŠ¸ë¡ 
### 2.3.2. ê°€ì¤‘ì¹˜ í¸í–¥
#### AND Gate
```python
import numpy as np
x = np.array([0,1]) #ì…ë ¥
w = np.array([0.5,0.5]) #ê°€ì¤‘ì¹˜
b = -0.7
'''
>> w*x
array([0, 0.5])
>> np.sum(w*x)
0.5
'''
tmp = np.sum(w*x) +b
if tmp <= 0:
    return 0
else
    return 1
```

#### NAND Gate
```python
def NAND(x1,x2):
    x = np.array([x1,x2])
    w = np.array([-0.5,-0.5])
    b = 0.7
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else
        return 1
```
#### OR
```python
def OR(x1,x2):
    x = np.array([x1,x2])
    w = np.array([0.5,0.5]) #ANDì™€ëŠ” ê°€ì¤‘ì¹˜ (wì™€ b)ë§Œ ë‹¤ë¥´ë‹¤!
    b = -0.2
    tmp = np.sum(w*x) + b
    if tmp <= 0:
        return 0
    else:
        return 1
```
AND,NAND, OR ëŠ” ëª¨ë‘ ê°™ì€ êµ¬ì¡°ì˜ í¼ì…‰íŠ¸ë¡ ì´ê³ , ì°¨ì´ëŠ” ê°€ì¤‘ì¹˜ ë§¤ê°œ ë³€ìˆ˜ì˜ ê°’ì´ë‹¤.
## 2.4. í¼ì…‰íŠ¸ë¡ ì˜ í•œê³„
AND,NAND,ORì˜ 3ê°€ì§€ ë…¼ë¦¬ íšŒë¡¤ë¥´ êµ¬í˜„ í•  ìˆ˜ ìˆìŒ.
### 2.4.1. XOR ê²Œì´íŠ¸
XOR ê²Œì´íŠ¸ëŠ” ë°°íƒ€ì  ë…¼ë¦¬í•©

|x_1|x_2|y|
|---|---|---|
|0|0|0|
|1|0|1|
|0|1|1|
|1|1|0|

-> ë‹¤ì¸µ í¼ì…‰íŠ¸ë¡  (multi-layer perceptron)ì„ ë§Œë“¤ì–´ì•¼í•¨.

![alt text](image-1.png)

|x_1|x_2|s_1|s_2|
|---|---|---|---|
|0|0|1|0|0
|1|0|1|1|1
|0|1|1|1|1
|1|1|0|1|0
```python
def XOR(x1,x2):
    s1 = NAND(x1,x2)
    s2 = OR(x1,x2)
    y = AND(s1,s2)
    return y
```
```
[ê²°ê³¼]
XOR(0,0) # 0ì„ ì¶œë ¥
XOR(1,0) # 1ì„ ì¶œë ¥
XOR(0,1) # 1ì„ ì¶œë ¥
XOR(1,1) # 0ì„ ì¶œë ¥
```
# 3.ì‹ ê²½ë§
### 3.2.3. ê³„ë‹¨ í•¨ìˆ˜ ê·¸ë˜í”„

```python
import numpy as np
import matplotlib.pyplot as plt
def step_function(x):
    return np.array(x > 0, dtype=int)
x = np.arange(-5.0, 5.0, 0.1)
y = step_function(x)
plt.plot(x, y)
plt.ylim(-0.1, 1.1)
plt.show()
```
![alt text](image-2.png)
### 3.2.4. ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ êµ¬í˜„í•˜ê¸°
```python
def sigmoid(x):
    return 1/(1+np.exp(-x))
```
![alt text](image-3.png)
### 3.2.6 ë¹„ì„ í˜•í•¨ìˆ˜
ì‹ ê²½ë§ì—ì„œëŠ” í™œì„±í™” í•¨ìˆ˜ë¡œ ë¹„ì…˜í˜• í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì•¼í•¨.
-> ì„ í˜•í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•´ì„œëŠ” ì•ˆë¨.

y?
>ì„ í˜•í•¨ìˆ˜ë¥¼ ì´ìš©í•˜ë©´ ì‹ ê²½ë§ì˜ ì¸µì„ ê¹Šê²Œí•˜ëŠ” ì˜ë¯¸ê°€ ì—†ì–´ì§.
### 3.2.7 ReLU í•¨ìˆ˜
ì…ë ¥ì´ 0ì„ ë„˜ìœ¼ë©´ ê·¸ ì…ë ¥ì„ ê·¸ëŒ€ë¡œ ì¶œë ¤í•˜ê³ , 0ì´í•˜ë©´ 0ì„ ì¶œë ¥
x (x>0)
0(x<=0)
```python
def relu(x):
    return np.maximum(0, x)
```
![alt text](image-5.png)
ë„˜íŒŒì´ì˜ maximumì„ ì‚¬ìš©
maximumì€ ë‘ ì…ë ¥ ì¤‘ í° ê°’ ì„ íƒ.

## 3.3 ë‹¤ì°¨ì› ë°°ì—´ ê³„ì‚°
```python
def init_network():
    network = {}
    network['W1'] = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])
    network['b1'] = np.array([0.1, 0.2, 0.3])
    network['W2'] = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])
    network['b2'] = np.array([0.1, 0.2])
    network['W3'] = np.array([[0.1, 0.3], [0.2, 0.4]])
    network['b3'] = np.array([0.1, 0.2])
    
    return network
def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    a3 = np.dot(z2, W3) + b3
    y = a3
    
    return y
network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)  # ì¶œë ¥ê°’ì´ 0.31682708, 0.69627909ê°€ ë‚˜ì˜´
```
## 3.5 ì¶œë ¥ì¸µ ì„¤ê³„í•˜ê¸°
```python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y
```
ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜êµ¬í˜„ ì‹œ ì£¼ì˜ì 

# 3ì¥ ì‹ ê²½ë§ - ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ì •ë¦¬

## 1. í¼ì…‰íŠ¸ë¡ ì—ì„œ ì‹ ê²½ë§ìœ¼ë¡œ

### í¼ì…‰íŠ¸ë¡  vs ì‹ ê²½ë§

```python
"""
í¼ì…‰íŠ¸ë¡ :
- ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •
- í™œì„±í™” í•¨ìˆ˜: ê³„ë‹¨ í•¨ìˆ˜ (0 or 1)

ì‹ ê²½ë§:
- ê°€ì¤‘ì¹˜ë¥¼ ë°ì´í„°ë¡œë¶€í„° ìë™ìœ¼ë¡œ í•™ìŠµ
- í™œì„±í™” í•¨ìˆ˜: ì‹œê·¸ëª¨ì´ë“œ, ReLU ë“± (ì—°ì†ì )
"""
```

### ì‹ ê²½ë§ì˜ êµ¬ì¡°

```
ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ â†’ ì¶œë ¥ì¸µ
(Input) (Hidden) (Output)

ì˜ˆ: 3ì¸µ ì‹ ê²½ë§
ì…ë ¥ì¸µ(2) â†’ ì€ë‹‰ì¸µ(3) â†’ ì¶œë ¥ì¸µ(2)
```

---

## 2. í™œì„±í™” í•¨ìˆ˜ â­â­â­

### 2.1 ê³„ë‹¨ í•¨ìˆ˜ (Step Function)

**ìˆ˜ì‹:**
$$h(x) = \begin{cases}
0, & (x \leq 0) \\
1, & (x > 0)
\end{cases}$$

**êµ¬í˜„:**
```python
import numpy as np

def step_function(x):
    return np.array(x > 0, dtype=int)

# í…ŒìŠ¤íŠ¸
x = np.array([-1.0, 0.0, 1.0, 2.0])
print(step_function(x))  # [0 0 1 1]
```

**íŠ¹ì§•:**
- ë¶ˆì—°ì† í•¨ìˆ˜
- ë¯¸ë¶„ ë¶ˆê°€ëŠ¥ (í•™ìŠµ ë¶ˆê°€)
- í¼ì…‰íŠ¸ë¡ ì—ì„œ ì‚¬ìš©

---

### 2.2 ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ (Sigmoid) â­â­â­

**ìˆ˜ì‹:**
$$h(x) = \frac{1}{1 + e^{-x}}$$

**êµ¬í˜„:**
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# í…ŒìŠ¤íŠ¸
x = np.array([-1.0, 0.0, 1.0, 2.0])
print(sigmoid(x))
# [0.26894142 0.5 0.73105858 0.88079708]
```

**íŠ¹ì§•:**
- ì—°ì† í•¨ìˆ˜
- ë¯¸ë¶„ ê°€ëŠ¥ (í•™ìŠµ ê°€ëŠ¥)
- ì¶œë ¥: 0 ~ 1 ì‚¬ì´
- ë¶€ë“œëŸ¬ìš´ ê³¡ì„ 

**ê·¸ë˜í”„ ë¹„êµ:**
```python
import matplotlib.pyplot as plt

x = np.arange(-5.0, 5.0, 0.1)
y_step = step_function(x)
y_sigmoid = sigmoid(x)

plt.plot(x, y_step, linestyle='--', label='Step')
plt.plot(x, y_sigmoid, label='Sigmoid')
plt.ylim(-0.1, 1.1)
plt.legend()
plt.show()
```

---

### 2.3 ReLU í•¨ìˆ˜ (Rectified Linear Unit) â­â­â­

**ìˆ˜ì‹:**
$$h(x) = \max(0, x) = \begin{cases}
x, & (x > 0) \\
0, & (x \leq 0)
\end{cases}$$

**êµ¬í˜„:**
```python
def relu(x):
    return np.maximum(0, x)

# í…ŒìŠ¤íŠ¸
x = np.array([-1.0, 0.0, 1.0, 2.0])
print(relu(x))  # [0. 0. 1. 2.]
```

**íŠ¹ì§•:**
- ì—°ì† í•¨ìˆ˜
- ê³„ì‚°ì´ ë§¤ìš° ê°„ë‹¨
- ê¸°ìš¸ê¸° ì†Œì‹¤ ë¬¸ì œ ì™„í™”
- **í˜„ëŒ€ ì‹ ê²½ë§ì—ì„œ ê°€ì¥ ë§ì´ ì‚¬ìš©**

---

### í™œì„±í™” í•¨ìˆ˜ ë¹„êµí‘œ

| í•¨ìˆ˜ | ìˆ˜ì‹ | ì¶œë ¥ ë²”ìœ„ | ë¯¸ë¶„ ê°€ëŠ¥ | ì‚¬ìš© ìœ„ì¹˜ |
|:----:|:-----|:---------:|:---------:|:----------|
| **Step** | $h(x) = \begin{cases}0 & (x \leq 0) \\ 1 & (x > 0)\end{cases}$ | {0, 1} | âŒ | í¼ì…‰íŠ¸ë¡  |
| **Sigmoid** | $h(x) = \frac{1}{1+e^{-x}}$ | (0, 1) | âœ… | ì€ë‹‰ì¸µ, ì´ì§„ë¶„ë¥˜ |
| **ReLU** | $h(x) = \max(0, x)$ | [0, âˆ) | âœ… | ì€ë‹‰ì¸µ (í˜„ëŒ€) |
| **Tanh** | $h(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$ | (-1, 1) | âœ… | ì€ë‹‰ì¸µ |

---

## 3. ë‹¤ì°¨ì› ë°°ì—´ ê³„ì‚° â­â­

### í–‰ë ¬ì˜ í˜•ìƒ

```python
import numpy as np

# 1ì°¨ì› ë°°ì—´
A = np.array([1, 2, 3, 4])
print(A.shape)  # (4,)
print(A.ndim)   # 1

# 2ì°¨ì› ë°°ì—´ (í–‰ë ¬)
B = np.array([[1, 2], [3, 4], [5, 6]])
print(B.shape)  # (3, 2) - 3í–‰ 2ì—´
print(B.ndim)   # 2

# 3ì°¨ì› ë°°ì—´
C = np.array([[[1, 2], [3, 4]], [[5, 6], [7, 8]]])
print(C.shape)  # (2, 2, 2)
print(C.ndim)   # 3
```

### í–‰ë ¬ì˜ ë‚´ì  (Dot Product) â­â­â­

```python
# í–‰ë ¬ ê³±ì…ˆ ê·œì¹™: (m, n) Ã— (n, k) = (m, k)

A = np.array([[1, 2], [3, 4]])      # (2, 2)
B = np.array([[5, 6], [7, 8]])      # (2, 2)
C = np.dot(A, B)
print(C)
# [[19 22]
#  [43 50]]
print(C.shape)  # (2, 2)

# í˜•ìƒì´ ë§ì§€ ì•Šìœ¼ë©´ ì˜¤ë¥˜
A = np.array([[1, 2, 3], [4, 5, 6]])  # (2, 3)
B = np.array([[1, 2], [3, 4]])        # (2, 2)
# np.dot(A, B)  # ì˜¤ë¥˜! (2,3) Ã— (2,2) ë¶ˆê°€ëŠ¥

# ì˜¬ë°”ë¥¸ ì˜ˆ
A = np.array([[1, 2, 3], [4, 5, 6]])  # (2, 3)
B = np.array([[1, 2], [3, 4], [5, 6]]) # (3, 2)
C = np.dot(A, B)                       # (2, 2) âœ…
print(C)
# [[22 28]
#  [49 64]]
```

**í•µì‹¬ ê·œì¹™:**
- **(m, n) Ã— (n, k) = (m, k)**
- ì• í–‰ë ¬ì˜ ì—´ ìˆ˜ = ë’¤ í–‰ë ¬ì˜ í–‰ ìˆ˜

---

## 4. 3ì¸µ ì‹ ê²½ë§ êµ¬í˜„ â­â­â­

### ì‹ ê²½ë§ êµ¬ì¡°

```
ì…ë ¥ì¸µ(2) â†’ ì€ë‹‰ì¸µ1(3) â†’ ì€ë‹‰ì¸µ2(2) â†’ ì¶œë ¥ì¸µ(2)
```

### ì „ì²´ êµ¬í˜„

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def identity_function(x):
    return x

def init_network():
    """ì‹ ê²½ë§ ì´ˆê¸°í™”"""
    network = {}
    
    # ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ1
    network['W1'] = np.array([[0.1, 0.3, 0.5],
                              [0.2, 0.4, 0.6]])  # (2, 3)
    network['b1'] = np.array([0.1, 0.2, 0.3])   # (3,)
    
    # ì€ë‹‰ì¸µ1 â†’ ì€ë‹‰ì¸µ2
    network['W2'] = np.array([[0.1, 0.4],
                              [0.2, 0.5],
                              [0.3, 0.6]])       # (3, 2)
    network['b2'] = np.array([0.1, 0.2])        # (2,)
    
    # ì€ë‹‰ì¸µ2 â†’ ì¶œë ¥ì¸µ
    network['W3'] = np.array([[0.1, 0.3],
                              [0.2, 0.4]])       # (2, 2)
    network['b3'] = np.array([0.1, 0.2])        # (2,)
    
    return network

def forward(network, x):
    """ìˆœì „íŒŒ"""
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    # ì…ë ¥ì¸µ â†’ ì€ë‹‰ì¸µ1
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    
    # ì€ë‹‰ì¸µ1 â†’ ì€ë‹‰ì¸µ2
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    # ì€ë‹‰ì¸µ2 â†’ ì¶œë ¥ì¸µ
    a3 = np.dot(z2, W3) + b3
    y = identity_function(a3)  # ë˜ëŠ” y = a3
    
    return y

# ì‹¤í–‰
network = init_network()
x = np.array([1.0, 0.5])
y = forward(network, x)
print(y)  # [0.31682708 0.69627909]
```

### í˜•ìƒ ì¶”ì 

```python
"""
ì…ë ¥: x (2,)

Layer 1:
  W1: (2, 3)
  b1: (3,)
  a1 = xÂ·W1 + b1 â†’ (2,)Â·(2,3) + (3,) = (3,)
  z1 = sigmoid(a1) â†’ (3,)

Layer 2:
  W2: (3, 2)
  b2: (2,)
  a2 = z1Â·W2 + b2 â†’ (3,)Â·(3,2) + (2,) = (2,)
  z2 = sigmoid(a2) â†’ (2,)

Layer 3:
  W3: (2, 2)
  b3: (2,)
  a3 = z2Â·W3 + b3 â†’ (2,)Â·(2,2) + (2,) = (2,)
  y = identity(a3) â†’ (2,)

ì¶œë ¥: y (2,)
"""
```

---

## 5. ì¶œë ¥ì¸µ ì„¤ê³„ â­â­â­

### 5.1 í•­ë“± í•¨ìˆ˜ (íšŒê·€ ë¬¸ì œ)

```python
def identity_function(x):
    return x

# ì¶œë ¥ì¸µ
y = identity_function(a3)
```

**ì‚¬ìš©:** íšŒê·€ ë¬¸ì œ (ì—°ì†ì ì¸ ê°’ ì˜ˆì¸¡)

---

### 5.2 ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ (ë¶„ë¥˜ ë¬¸ì œ) â­â­â­

**ìˆ˜ì‹:**
$$y_k = \frac{\exp(a_k)}{\displaystyle\sum_{i=1}^{n}\exp(a_i)}$$

**ê¸°ë³¸ êµ¬í˜„:**
```python
def softmax(a):
    exp_a = np.exp(a)
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

a = np.array([0.3, 2.9, 4.0])
y = softmax(a)
print(y)           # [0.01821127 0.24519181 0.73659691]
print(np.sum(y))   # 1.0
```

**ì˜¤ë²„í”Œë¡œ ë°©ì§€ ë²„ì „:**
```python
def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)  # ì˜¤ë²„í”Œë¡œ ëŒ€ì±…
    sum_exp_a = np.sum(exp_a)
    y = exp_a / sum_exp_a
    return y

# í° ê°’ì—ì„œë„ ì•ˆì „
a = np.array([1010, 1000, 990])
print(softmax(a))
# [9.99954600e-01 4.53978686e-05 2.06106005e-09]
```

**íŠ¹ì§•:**
- ì¶œë ¥ í•©ì´ 1 (í™•ë¥ ë¡œ í•´ì„ ê°€ëŠ¥)
- ê° ì›ì†ŒëŠ” 0 ~ 1 ì‚¬ì´
- ì›ì†Œì˜ ëŒ€ì†Œ ê´€ê³„ ìœ ì§€

**ì‚¬ìš©:**
- ë‹¤ì¤‘ í´ë˜ìŠ¤ ë¶„ë¥˜ ë¬¸ì œ
- í•™ìŠµ ë‹¨ê³„ì—ì„œ ì‚¬ìš©
- ì¶”ë¡  ë‹¨ê³„ì—ì„œëŠ” argmaxë§Œ ì‚¬ìš©

---

### ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ

| ë¬¸ì œ ìœ í˜• | í™œì„±í™” í•¨ìˆ˜ | ì¶œë ¥ |
|:---------|:-----------|:-----|
| íšŒê·€ | í•­ë“± í•¨ìˆ˜ | ì‹¤ìˆ˜ |
| ì´ì§„ ë¶„ë¥˜ | Sigmoid | 0 ~ 1 (í™•ë¥ ) |
| ë‹¤ì¤‘ ë¶„ë¥˜ | Softmax | 0 ~ 1 (í™•ë¥  ë¶„í¬) |

---

## 6. ë°°ì¹˜ ì²˜ë¦¬ â­â­â­

### ë°°ì¹˜ ì²˜ë¦¬ì˜ í•„ìš”ì„±

```python
# ë‹¨ì¼ ì…ë ¥ ì²˜ë¦¬ (ëŠë¦¼)
for x in dataset:
    y = network.predict(x)

# ë°°ì¹˜ ì²˜ë¦¬ (ë¹ ë¦„)
y_batch = network.predict(x_batch)
```

**ì¥ì :**
- í–‰ë ¬ ì—°ì‚° ìµœì í™”
- GPU ë³‘ë ¬ ì²˜ë¦¬ ê°€ëŠ¥
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

### ë°°ì¹˜ ì²˜ë¦¬ êµ¬í˜„

```python
# ë‹¨ì¼ ì…ë ¥
x = np.array([1.0, 0.5])
print(x.shape)  # (2,)

# ë°°ì¹˜ ì…ë ¥ (100ê°œ)
x_batch = np.array([[1.0, 0.5],
                    [0.2, 0.8],
                    [0.5, 0.3]])
print(x_batch.shape)  # (3, 2)

# ìˆœì „íŒŒëŠ” ë™ì¼í•˜ê²Œ ì‘ë™
y_batch = forward(network, x_batch)
print(y_batch.shape)  # (3, 2)
```

### í˜•ìƒ ë³€í™”

```python
"""
ì…ë ¥: (ë°°ì¹˜í¬ê¸°, ì…ë ¥í¬ê¸°)
ì˜ˆ: (100, 784)

W1: (784, 50)
ì¶œë ¥1: (100, 50)

W2: (50, 10)
ì¶œë ¥2: (100, 10)

ìµœì¢…: (ë°°ì¹˜í¬ê¸°, ì¶œë ¥í¬ê¸°)
"""
```

---

## 7. MNIST ì†ê¸€ì”¨ ì¸ì‹ ì˜ˆì œ â­â­

### ë°ì´í„° ë¡œë“œ

```python
from dataset.mnist import load_mnist

# ë°ì´í„° ë¡œë“œ
(x_train, t_train), (x_test, t_test) = load_mnist(
    normalize=True,      # 0~1ë¡œ ì •ê·œí™”
    flatten=True,        # 1ì°¨ì› ë°°ì—´ë¡œ ë³€í™˜
    one_hot_label=False  # ë ˆì´ë¸”ì„ ìˆ«ìë¡œ
)

print(x_train.shape)  # (60000, 784)
print(t_train.shape)  # (60000,)
print(x_test.shape)   # (10000, 784)
print(t_test.shape)   # (10000,)
```

### ì‹ ê²½ë§ êµ¬í˜„

```python
import pickle

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    c = np.max(x, axis=-1, keepdims=True)
    exp_x = np.exp(x - c)
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

def get_data():
    (x_train, t_train), (x_test, t_test) = load_mnist(
        normalize=True, flatten=True, one_hot_label=False
    )
    return x_test, t_test

def init_network():
    """í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ"""
    with open("sample_weight.pkl", 'rb') as f:
        network = pickle.load(f)
    return network

def predict(network, x):
    """ìˆœì „íŒŒ"""
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)
    
    return y

# ì •í™•ë„ ê³„ì‚°
x, t = get_data()
network = init_network()

accuracy_cnt = 0
for i in range(len(x)):
    y = predict(network, x[i])
    p = np.argmax(y)  # í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì›ì†Œì˜ ì¸ë±ìŠ¤
    if p == t[i]:
        accuracy_cnt += 1

print(f"Accuracy: {float(accuracy_cnt) / len(x)}")
```

### ë°°ì¹˜ ì²˜ë¦¬ ë²„ì „

```python
x, t = get_data()
network = init_network()

batch_size = 100
accuracy_cnt = 0

for i in range(0, len(x), batch_size):
    x_batch = x[i:i+batch_size]
    y_batch = predict(network, x_batch)
    
    # axis=1: ê° ë°ì´í„°(í–‰)ì—ì„œ ìµœëŒ“ê°’ì˜ ì¸ë±ìŠ¤
    p = np.argmax(y_batch, axis=1)
    accuracy_cnt += np.sum(p == t[i:i+batch_size])

print(f"Accuracy: {float(accuracy_cnt) / len(x)}")
```

---

## 8. ì‹œí—˜ ì˜ˆìƒ ë¬¸ì œ

### ë¬¸ì œ 1 (10ì )
```
ë‹¤ìŒ í–‰ë ¬ ê³±ì…ˆì˜ ê²°ê³¼ í˜•ìƒì„ êµ¬í•˜ì‹œì˜¤.

(1) A(2, 3) Ã— B(3, 4) = ?
(2) A(5, 2) Ã— B(2, 1) = ?
(3) A(10, 784) Ã— B(784, 50) = ?
```

**ë‹µì•ˆ:**
```python
(1) (2, 4)
(2) (5, 1)
(3) (10, 50)

# ê·œì¹™: (m, n) Ã— (n, k) = (m, k)
```

---

### ë¬¸ì œ 2 (10ì )
```
ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ì˜ ìˆ˜ì‹ì„ ì“°ê³ , ì¶œë ¥ ë²”ìœ„ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.
```

**ë‹µì•ˆ:**
```
ìˆ˜ì‹: h(x) = 1 / (1 + exp(-x))

ì¶œë ¥ ë²”ìœ„: 0 < h(x) < 1
- x â†’ -âˆì¼ ë•Œ, h(x) â†’ 0
- x = 0ì¼ ë•Œ, h(x) = 0.5
- x â†’ âˆì¼ ë•Œ, h(x) â†’ 1

íŠ¹ì§•: ì—°ì†ì ì´ê³  ë¯¸ë¶„ ê°€ëŠ¥í•˜ì—¬ í•™ìŠµì— ì‚¬ìš© ê°€ëŠ¥
```

---

### ë¬¸ì œ 3 (15ì )
```
ë‹¤ìŒ ì‹ ê²½ë§ì˜ ìˆœì „íŒŒ ê³¼ì •ì—ì„œ ê° ì¸µì˜ ì¶œë ¥ í˜•ìƒì„ êµ¬í•˜ì‹œì˜¤.

ì…ë ¥: x (100, 784)
W1: (784, 256), b1: (256,)
í™œì„±í™”: ReLU
W2: (256, 128), b2: (128,)
í™œì„±í™”: ReLU
W3: (128, 10), b3: (10,)
í™œì„±í™”: Softmax
```

**ë‹µì•ˆ:**
```python
ì…ë ¥: (100, 784)

Layer 1:
  a1 = xÂ·W1 + b1
  í˜•ìƒ: (100, 784)Â·(784, 256) + (256,) = (100, 256)
  z1 = ReLU(a1)
  í˜•ìƒ: (100, 256)

Layer 2:
  a2 = z1Â·W2 + b2
  í˜•ìƒ: (100, 256)Â·(256, 128) + (128,) = (100, 128)
  z2 = ReLU(a2)
  í˜•ìƒ: (100, 128)

Layer 3:
  a3 = z2Â·W3 + b3
  í˜•ìƒ: (100, 128)Â·(128, 10) + (10,) = (100, 10)
  y = Softmax(a3)
  í˜•ìƒ: (100, 10)

ìµœì¢… ì¶œë ¥: (100, 10)
```

---

### ë¬¸ì œ 4 (10ì )
```
Softmax í•¨ìˆ˜ì˜ íŠ¹ì§• 3ê°€ì§€ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.
```

**ë‹µì•ˆ:**
```
1. ì¶œë ¥ì˜ í•©ì´ 1
   - Î£y_i = 1ì´ë¯€ë¡œ í™•ë¥ ë¡œ í•´ì„ ê°€ëŠ¥

2. ì¶œë ¥ ë²”ìœ„ê°€ 0~1
   - ê° ì›ì†Œê°€ 0 < y_i < 1

3. ì…ë ¥ì˜ ëŒ€ì†Œ ê´€ê³„ ìœ ì§€
   - exp(x)ê°€ ë‹¨ì¡°ì¦ê°€ í•¨ìˆ˜ì´ë¯€ë¡œ
   - argmax(ì…ë ¥) = argmax(ì¶œë ¥)
   
ë”°ë¼ì„œ ì¶”ë¡  ì‹œì—ëŠ” Softmaxë¥¼ ìƒëµí•˜ê³  
argmaxë§Œ ì‚¬ìš©í•´ë„ ë¨
```

---

### ë¬¸ì œ 5 (15ì )
```
ë‹¤ìŒ ì½”ë“œì˜ ì¶œë ¥ì„ ì˜ˆì¸¡í•˜ê³ , ê° ë‹¨ê³„ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.

```python
import numpy as np

def softmax(a):
    c = np.max(a)
    exp_a = np.exp(a - c)
    return exp_a / np.sum(exp_a)

a = np.array([1.0, 2.0, 3.0])
y = softmax(a)
print(y)
print(np.sum(y))
print(np.argmax(y))
```
```

**ë‹µì•ˆ:**
```python
# ì¶œë ¥ ì˜ˆì¸¡
y â‰ˆ [0.09003057, 0.24472847, 0.66524096]
np.sum(y) = 1.0
np.argmax(y) = 2

# ì„¤ëª…
1. c = 3.0 (ìµœëŒ“ê°’)
2. a - c = [-2.0, -1.0, 0.0]
3. exp(a - c) = [0.13533528, 0.36787944, 1.0]
4. sum = 1.50321472
5. y = exp(a - c) / sum
6. í•©ì€ 1.0 (í™•ë¥  ë¶„í¬)
7. ìµœëŒ“ê°’ ì¸ë±ìŠ¤ëŠ” 2 (3.0ì´ ê°€ì¥ í¼)
```

---

## 9. í•µì‹¬ ê°œë… ì •ë¦¬

### âœ… ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ê²ƒ

1. **í™œì„±í™” í•¨ìˆ˜**
   - ê³„ë‹¨ í•¨ìˆ˜: í¼ì…‰íŠ¸ë¡ , ë¶ˆì—°ì†
   - ì‹œê·¸ëª¨ì´ë“œ: 0~1, ì—°ì†, ë¯¸ë¶„ ê°€ëŠ¥
   - ReLU: max(0, x), í˜„ëŒ€ì , ë¹ ë¦„

2. **í–‰ë ¬ ê³±ì…ˆ**
   - (m, n) Ã— (n, k) = (m, k)
   - ì• ì—´ ìˆ˜ = ë’¤ í–‰ ìˆ˜

3. **ìˆœì „íŒŒ**
   - a = xÂ·W + b (ê°€ì¤‘í•©)
   - z = h(a) (í™œì„±í™”)
   - ì¸µë§ˆë‹¤ ë°˜ë³µ

4. **ì¶œë ¥ì¸µ**
   - íšŒê·€: í•­ë“± í•¨ìˆ˜
   - ì´ì§„ ë¶„ë¥˜: Sigmoid
   - ë‹¤ì¤‘ ë¶„ë¥˜: Softmax

5. **ë°°ì¹˜ ì²˜ë¦¬**
   - ì—¬ëŸ¬ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬
   - ì²« ë²ˆì§¸ ì°¨ì›ì´ ë°°ì¹˜ í¬ê¸°
   - axis=1ë¡œ ê° ìƒ˜í”Œë³„ ì²˜ë¦¬

---

## 10. ë¹ ë¥¸ ë³µìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] í™œì„±í™” í•¨ìˆ˜ 3ê°€ì§€ (ê³„ë‹¨, ì‹œê·¸ëª¨ì´ë“œ, ReLU)
- [ ] ê° í•¨ìˆ˜ì˜ ìˆ˜ì‹ê³¼ íŠ¹ì§•
- [ ] í–‰ë ¬ ê³±ì…ˆ í˜•ìƒ ê³„ì‚°
- [ ] ìˆœì „íŒŒ ê³¼ì • ì´í•´
- [ ] ì‹ ê²½ë§ êµ¬í˜„ ì½”ë“œ ì‘ì„± ê°€ëŠ¥
- [ ] ì¶œë ¥ì¸µ í™œì„±í™” í•¨ìˆ˜ ì„ íƒ ê¸°ì¤€
- [ ] Softmax í•¨ìˆ˜ì™€ íŠ¹ì§•
- [ ] ë°°ì¹˜ ì²˜ë¦¬ì˜ ì¥ì 
- [ ] axis ë§¤ê°œë³€ìˆ˜ ì´í•´
- [ ] MNIST ì˜ˆì œ íë¦„ ì´í•´

---

## 11. ì½”ë“œ í…œí”Œë¦¿ (ì•”ê¸°ìš©)

### 3ì¸µ ì‹ ê²½ë§ ê¸°ë³¸ í…œí”Œë¦¿

```python
import numpy as np

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def softmax(x):
    c = np.max(x, axis=-1, keepdims=True)
    exp_x = np.exp(x - c)
    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)

def init_network():
    network = {}
    network['W1'] = np.random.randn(ì…ë ¥, ì€ë‹‰1)
    network['b1'] = np.zeros(ì€ë‹‰1)
    network['W2'] = np.random.randn(ì€ë‹‰1, ì€ë‹‰2)
    network['b2'] = np.zeros(ì€ë‹‰2)
    network['W3'] = np.random.randn(ì€ë‹‰2, ì¶œë ¥)
    network['b3'] = np.zeros(ì¶œë ¥)
    return network

def forward(network, x):
    W1, W2, W3 = network['W1'], network['W2'], network['W3']
    b1, b2, b3 = network['b1'], network['b2'], network['b3']
    
    a1 = np.dot(x, W1) + b1
    z1 = sigmoid(a1)
    
    a2 = np.dot(z1, W2) + b2
    z2 = sigmoid(a2)
    
    a3 = np.dot(z2, W3) + b3
    y = softmax(a3)
    
    return y
```

**ì‹œí—˜ í™”ì´íŒ…! ğŸš€**
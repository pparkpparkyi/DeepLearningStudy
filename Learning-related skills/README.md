
# 🚀 신경망 학습 관련 주요 기술 정리


## 📘 1️⃣ 가중치 초기화 (Weight Initialization)

### ⚙️ 왜 중요한가?
- 잘못된 초기값은 학습 초기에 **출력값 폭발(exploding)** 또는 **소실(vanishing)** 을 유발
- 각 층의 출력 분산을 일정하게 유지하는 것이 목표

### 🧩 대표 기법

| 초기화 방법 | 수식 | 특징 | 적용 함수 |
|:-------------|:-----------------------------|:------------------|:------------------|
| **Xavier 초기화** | \( Var(W) = \frac{2}{n_{in} + n_{out}} \) | 입력·출력 수 모두 고려 | `sigmoid`, `tanh` |
| **He 초기화** | \( Var(W) = \frac{2}{n_{in}} \) | ReLU용, 큰 분산 유지 | `ReLU`, `LeakyReLU` |
| **표준정규분포 초기화** | \( W \sim \mathcal{N}(0,1) \) | 단순하지만 비안정적 | 실습 초보용 |

📌 **핵심 포인트**  
> 가중치 초기화의 목적은 **출력 분산을 일정하게 유지**하여  
> **기울기 소실/폭발을 방지**하는 것!

---

## ⚡ 2️⃣ 배치 정규화 (Batch Normalization)

### 💡 개념
각 층의 입력 분포가 계속 변하는 문제(Internal Covariate Shift)를 해결하기 위해  
입력값을 정규화(normalize)하여 **학습을 안정화**하는 기법

\[
\hat{x} = \frac{x - \mu_B}{\sqrt{\sigma_B^2 + \epsilon}} \quad , \quad y = \gamma \hat{x} + \beta
\]

### ✅ 장점
- 학습 속도 향상 (더 큰 learning rate 가능)
- 초기값 의존성 감소
- Gradient 폭발/소실 완화
- Regularization 효과 (약한 Dropout 역할)

📌 **요약**
> 각 층의 입력을 평균 0, 분산 1로 맞추어 **안정된 학습을 보장**

---

## 💧 3️⃣ Dropout (드롭아웃)

### 💡 개념
학습 시 **일부 뉴런을 확률적으로 비활성화(0)** 시켜  
특정 뉴런에 과도하게 의존하지 않도록 하는 Regularization 기법

\[
h' = h \times mask
\]

(mask는 확률 p로 뉴런을 유지하는 이진 벡터)

### ✅ 장점
- 과적합(Overfitting) 방지  
- 신경망이 **더 일반화된 표현**을 학습  

### ⚙️ 예시
```python
mask = np.random.rand(*h.shape) > dropout_ratio
h *= mask
```

📌 **핵심 포인트**

> Dropout은 **훈련 시 랜덤 비활성화**,
> **테스트 시 스케일 조정**으로 평균 출력을 맞춤

---

## 📉 4️⃣ 최적화 알고리즘 (Optimizers)

### 💡 기본 원리

손실함수 ( L )을 최소화하기 위해
기울기(Gradient)를 이용해 파라미터를 갱신하는 방법

[
W = W - \eta \frac{∂L}{∂W}
]

---

### 🧠 대표 기법 정리

| Optimizer                             | 핵심 개념                 | 수식/특징                                        | 비고             |
| :------------------------------------ | :-------------------- | :------------------------------------------- | :------------- |
| **SGD (Stochastic Gradient Descent)** | 확률적 경사하강법             | ( W = W - \eta \nabla W )                    | 단순, 안정적이나 느림   |
| **Momentum**                          | 이전 기울기 관성 유지          | ( v = \alpha v - \eta \nabla W ), ( W += v ) | 흔들림 감소         |
| **AdaGrad**                           | 학습률 자동 조정             | ( W = W - \frac{\eta}{\sqrt{h}} \nabla W )   | 희소데이터에 효과적     |
| **RMSProp**                           | AdaGrad 개선 (지속학습)     | 이동평균으로 학습률 조정                                | RNN에 적합        |
| **Adam**                              | Momentum + RMSProp 결합 | ( m_t, v_t ) 두 모멘텀 사용                        | 가장 많이 쓰이는 최적화기 |
|                                       |                       |                                              |                |

📌 **Adam의 핵심 하이퍼파라미터**

| 파라미터   | 의미        | 기본값   |
| :----- | :-------- | :---- |
| α (lr) | 학습률       | 0.001 |
| β₁     | 1차 모멘텀 계수 | 0.9   |
| β₂     | 2차 모멘텀 계수 | 0.999 |
| ϵ      | 수치 안정화 상수 | 1e-8  |

---

## 🎯 5️⃣ 학습률 스케줄링 (Learning Rate Scheduling)

### 💡 개념

* 학습이 진행될수록 **학습률을 점진적으로 감소**시켜
  더 안정적인 수렴을 유도하는 기법

| 방식                    | 설명                                  |
| :-------------------- | :---------------------------------- |
| **Step Decay**        | 일정 주기마다 학습률 감소                      |
| **Exponential Decay** | 지수 함수 형태로 감소                        |
| **Cosine Annealing**  | 주기적으로 감소했다 다시 증가                    |
| **Warmup + Decay**    | 초반에는 천천히 증가 → 이후 감소 (Transformer 등) |

📌 **요약**

> Learning Rate는 일정하지 않아야 한다.
> → 처음엔 빠르게, 후반엔 안정적으로 학습.

---

## 🧮 6️⃣ 기울기 소실 / 폭발 방지 (Gradient Vanishing / Exploding)

| 문제                     | 원인                                 | 해결책                        |
| :--------------------- | :--------------------------------- | :------------------------- |
| **Gradient Vanishing** | sigmoid, tanh 등에서 gradient가 0으로 수렴 | ReLU 사용, He 초기화, BatchNorm |
| **Gradient Exploding** | 깊은 네트워크에서 gradient가 너무 커짐          | Gradient Clipping, 작은 학습률  |

📌 **핵심**

> “학습이 안 된다”는 대부분의 원인은
> **Gradient 흐름이 왜곡되기 때문**이다.

---

## 🧠 7️⃣ 데이터 관련 기법

| 기법                                  | 설명                           | 효과             |
| :---------------------------------- | :--------------------------- | :------------- |
| **Data Augmentation**               | 회전, 자르기, 좌우반전 등 데이터 변형       | 데이터 다양성↑, 과적합↓ |
| **Normalization / Standardization** | 입력 데이터 분포를 평균 0, 표준편차 1로 정규화 | 학습 안정화         |
| **Shuffling**                       | 미니배치 구성 시 순서 무작위화            | 편향된 학습 방지      |

---

## 💡 8️⃣ 정리 요약

| 분야         | 대표 기술                        | 핵심 목적             |
| :--------- | :--------------------------- | :---------------- |
| **초기화**    | Xavier / He                  | Gradient 안정화      |
| **정규화**    | BatchNorm                    | 내부 분포 고정, 학습속도 향상 |
| **규제**     | Dropout / L2                 | 과적합 방지            |
| **최적화**    | Adam / SGD / Momentum        | 빠르고 안정적인 수렴       |
| **학습 제어**  | LR Scheduling                | 효율적인 학습률 관리       |
| **데이터 처리** | Normalization / Augmentation | 학습 데이터 품질 향상      |

---

## 🚀 한 줄 요약

> 신경망 학습의 성패는 **하이퍼파라미터와 학습 기술**에 달려 있다.
>
> “가중치 초기화 → 정규화 → 최적화 → 규제 → 학습률 관리”
> 이 다섯 축을 균형 있게 적용해야
> **안정적이고 빠른 학습**이 가능하다.


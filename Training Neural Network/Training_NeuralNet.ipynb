{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ad9b45",
   "metadata": {},
   "source": [
    "# ì‹ ê²½ë§ í•™ìŠµ\n",
    "**í•™ìŠµ** : í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ìµœì ê°’   \n",
    "[ì†ì‹¤í•¨ìˆ˜] : ì‹ ê²½ë§ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6a320",
   "metadata": {},
   "source": [
    "## 1. ì˜¤ì°¨ ì œê³± í•©\n",
    "sum of squares for error (SSE)\n",
    "* ì›í•« ì¸ì½”ë”©ì„ í•¨.\n",
    "\n",
    "$$ E = \\frac{1}{2} \\displaystyle\\sum_k(y_k-t_k)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85ddd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares_error(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d56d6",
   "metadata": {},
   "source": [
    "## 2. Cross Entropy Error(CEE)   \n",
    "êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨\n",
    "\n",
    "$$E = -\\displaystyle\\sum_k\\;t_k\\;\\text{log}y_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d1b82",
   "metadata": {},
   "source": [
    "deltaë¥¼ ë”í•˜ëŠ” ì´ìœ ëŠ” np.log()í•¨ìˆ˜ì—¥ 0ì„ ì…ë ¥í•˜ë©´ ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€.\n",
    "ë”°ë¼ì„œ ì•„ì£¼ ì‘ì€ ê°’ì„ ë”í•´ì„œ 0ì´ ë  ìˆ˜ ì—†ê²Œ í–ˆìŒ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfd58c",
   "metadata": {},
   "source": [
    "## 3. ë¯¸ë‹ˆë°°ì¹˜\n",
    "$$ E= -\\frac {1}{N} \\displaystyle\\sum_n\\displaystyle\\sum_k\\; t_{nk}\\; \\text{log}\\;y_{nk} $$\n",
    "\n",
    "ì¼ë¶€ë§Œ ê³¨ë¼ì„œ í•™ìŠµì„ ìˆ˜í–‰í•˜ëŠ”ê²ƒ.  \n",
    "**ë¯¸ë‹ˆë°°ì¹˜**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53393da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'..'))\n",
    "import numpy as np\n",
    "from dataset.mnist import loasd_mnist\n",
    "\n",
    "(x_train,t_train),(x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "print(x_train.shape) # (60000,784)\n",
    "print(t_train.shape) # (60000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7f393",
   "metadata": {},
   "source": [
    "í˜¸ì¶œí•  ë•Œ one_hot_label=Trueë¡œ ì§€ì •í•˜ì—¬ ì›í•«ì¸ì½”ë”©ìœ¼ë¡œ,   \n",
    "ì•ì˜ ì½”ë“œì—ì„œ MNIST ë°ì´í„°ë¥¼ ì½ì€ ê²°ê³¼,    \n",
    "í›ˆë ¨ë°ì´í„°ëŠ” 60,000ê°œê³  ì…ë ¥ ë°ì´í„°ëŠ” 784ì—´ (ì›ë˜ëŠ” 28x28)ì¸ ì´ë¯¸ì§€ ë°ì´í„°ì„ì„ í•  ìˆ˜ ìˆë‹¤.     \n",
    "ì •ë‹µë ˆì´ë¸”ì€ 10ì¤„ì§œë¦¬ ë°ì´í„°ì„."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3fbe8",
   "metadata": {},
   "source": [
    "### ì™œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì„¤ì •í•˜ëŠ”ê°€?\n",
    "ì‹ ê²½ë§ì„ í•™ìŠµí•  ë•Œ ì •í™•ë„ë¥¼ ì§€í‘œë¡œ ì‚¼ì•„ì„œëŠ” ì•ˆ ëœë‹¤.  \n",
    "ì •í™•ë„ë¥¼ ì§€í‘œë¡œ í•˜ë©´ ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ë¶„ì´ ëŒ€ë¶€ë¶„ì˜ ì¥ì†Œì—ì„œ 0ì´ ë˜ê¸° ë•Œë¬¸ì´ë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a1175",
   "metadata": {},
   "source": [
    "## 4. ë¯¸ë¶„\n",
    "í•´ì„ì ë¯¸ë¶„   \n",
    "$\\frac{df(x)}{dx}=\\displaystyle\\lim_{h\\rarr\\infin}{\\frac{f(x+h)-f(x)}{h}}$\n",
    "-> ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ ë¬¸ì œ ì¼ìœ¼í‚´   \n",
    "ìˆ˜ì¹˜ë¯¸ë¶„ numerical differentiation   \n",
    "**ì¤‘ì‹¬ì°¨ë¶„** **ì¤‘ì•™ì°¨ë¶„**ìœ¼ë¡œ í•´ê²°   \n",
    "$\\frac{df(x)}{dx}=\\displaystyle\\lim_{h\\rarr\\infin}{\\frac{f(x+h)-f(x-h)}{2h}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583d4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=1e-4 # 0.0001\n",
    "    return(f(x+h)-f(x-h)/(2*h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad22af",
   "metadata": {},
   "source": [
    "### í¸ë¯¸ë¶„\n",
    "\n",
    "$f(x_0,x_1)\\;=\\;x^2_0+x^2_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfabe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "#ë˜ëŠ” return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a69b35",
   "metadata": {},
   "source": [
    "ë¬¸ì œ: $x_0 = 3, x_1 = 4$ ì¼ë•Œ, $x_0$ ì— ëŒ€í•œ í¸ë¯¸ë¶„ $\\frac{\\partial{f}}{\\partial{x_0}}$ ë¥¼ êµ¬í•˜ë¼."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4150b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64983.99944998999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 +4.0**2**0\n",
    "\n",
    "numerical_diff(function_tmp1,3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a7f34",
   "metadata": {},
   "source": [
    "## 5. ê¸°ìš¸ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49591aba",
   "metadata": {},
   "source": [
    "ê¸°ìš¸ê¸°ê°€ ê°€ë¥´í‚¤ëŠ” ìª½ì€ ê° ì¥ì†Œì—ì„œ í•¨ìˆ˜ì˜ ì¶œë ¥ ê°’ì„ ê°€ì¥ í¬ê²Œ ì¤„ì´ëŠ” ë°©í–¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a29cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # xì™€ í˜•ìƒì´ ê°™ì€ ë°°ì—´ì„ ìƒì„±\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) ê³„ì‚°\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) ê³„ì‚°\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val #ê°’ ë³µì›\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40843ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#í•™ìŠµë¥ \n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "    return x\n",
    "# ì¸ìˆ˜ fëŠ” ìµœì í™” í•˜ë ¤ëŠ” í•¨ìˆ˜, init_xëŠ” ì´ˆê¹ƒê°’, lrì€ learning rateë¥¼ ì˜ë¯¸. step_numì€ ê²½ì‚¬ë²•ì— ë”°ë¥¸ ë°˜ë³µ íšŸìˆ˜\n",
    "# í•™ìŠµë¥ ì„ ê³±í•œ ê°’ìœ¼ë¡œ ê°±ì‹ í•˜ëŠ” ì²˜ë¦¬ë¥¼ step_numë²ˆ ë°˜ë³µ\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed7089",
   "metadata": {},
   "source": [
    "ì‹ ê²½ë§ì—ì„œë„ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•´ì•¼í•¨.   \n",
    "ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì†ì‹¤í•¨ìˆ˜ì— ëŒ€í•œ ê¸°ìš¸ê¸°.\n",
    "### simpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpu as np\n",
    "import sys,os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'..'))\n",
    "from common.function import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #ì •ê·œë¶„í¬ë¡œ ì´ˆê¸°í™”\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae6830",
   "metadata": {},
   "source": [
    "common/functions.pyì— ì •ì˜í•œ softmaxì™€ cross_entropy_error ë§¤ì„œë“œë¥¼ ì˜ìš©í•¨   \n",
    "ê·¸ë¦¬ê³  common.gradient.pyì— ì •ì˜í•œ numerical_gradientë©”ì„œë“œë„ ì´ìš©.  \n",
    "\n",
    "simpleNet í´ë˜ìŠ¤ëŠ” í˜•ìƒì´ 2x3ì¸ 2ì°¨ì› ë°°ì—´    \n",
    "\n",
    "dWì˜ ë‚´ìš©ì„ ë³´ë©´, $\\frac{\\partial L}{\\partial W}$ ì˜ $\\frac{\\partial L}{\\partial w_{11}}$ ì€ ëŒ€ëµ 0.2 ì´ë‹¤.  \n",
    "ì´ëŠ”   $w_{11}$ ì„ $h$ ë§Œí¼ ëŠ˜ë¦¬ë©´ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ $0.2h$ ë§Œí¼ ì¦ê°€í•¨ì„ ì˜ë¯¸.   \n",
    "\n",
    "ë§ˆì°¬ê°€ì§€ë¡œ $\\frac{\\partial L}{\\partial w_{23}}$ ì€ ëŒ€ëµ -0.5ì´ë‹ˆ,  $w_{23}$ ì„ $h$ ë§Œí¼ ëŠ˜ë¦¬ë©´ ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì€ $0.5h$ ë§Œí¼ ê°ì†Œí•¨ì„ ì˜ë¯¸.   \n",
    "\n",
    "$\\therefore w_{23} ì´ w_{11} ë³´ë‹¤ ë” í¬ê²Œ ê¸°ì—¬í•¨$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127af632",
   "metadata": {},
   "source": [
    "### í•™ìŠµ ì•Œê³ ë¦¬ì¦˜\n",
    "ì „ì²´\n",
    "* ì‹ ê²½ë§ì—ëŠ” ì ì‘ ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì´ ìˆê³ , ì´ ê°€ì¤‘ì¹˜ì™€ í¸í–¥ã…‡ë¥´ í›ˆë ¨ ë°ì´í„°ì— ì ì‘í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ê³¼ì •ì„ í•™ìŠµì´ë¼í•¨.   \n",
    "* ì‹ ê²½ë§ í•™ìŠµì€ 4ë‹¨ê³„ë¡œ ìˆ˜í–‰\n",
    "1. ë¯¸ë‹ˆë°°ì¹˜   \n",
    "* í›ˆë ¨ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ê°€ì ¸ì˜´.\n",
    "* ì„ ë³„ëœ ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¼ í•˜ë©°, ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ëŠ” ê±°ì‹œ ëª©í‘œ\n",
    "2. ê¸°ìš¸ê¸° ì‚°ì¶œ\n",
    "* ë¯¸ë‹ˆë°°ì¹˜ì˜ ì†ì‹¤ í•¨ìˆ˜ ê°’ì„ ì¤„ì´ê¸° ìœ„í•´ ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•¨.\n",
    "* ê¸°ìš¸ê¸°ëŠ” ì†ì‹¤ í•¨ìˆ˜ì˜ ê°’ì„ ê°€ì¥ ì‘ê²Œ í•˜ëŠ” ë°©í–¥\n",
    "3. ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \n",
    "* ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì•„ì£¼ ì¡°ê¸ˆ ê°±ì‹ í•¨.\n",
    "4. ë°˜ë³µ\n",
    "* 1~3ë‹¨ê³„ ë°˜ë³µ\n",
    "\n",
    "ì´ ë•Œ, ë°ì´í„°ë¥¼ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë¬´ì‘ìœ„ë¡œ ì„ ì •í•˜ê¸° ë•Œë¬¸ì— ê²½ì‚¬í•˜ê°•ë²•ì€ í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²• SGD ë¼ê³  ë¶€ë¦„   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a47f35",
   "metadata": {},
   "source": [
    "ê²½ì‚¬ë²•ì€ ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ëŠ”ê±°ì„."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbccb7",
   "metadata": {},
   "source": [
    "# 4ì¥ ì‹ ê²½ë§ í•™ìŠµ - ì‹œí—˜ ëŒ€ë¹„ í•µì‹¬ ì •ë¦¬\n",
    "\n",
    "## 1. í•™ìŠµì˜ ê°œë… â­â­â­\n",
    "\n",
    "### í•™ìŠµì´ë€?\n",
    "\n",
    "**í›ˆë ¨ ë°ì´í„°ë¡œë¶€í„° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì˜ ìµœì ê°’ì„ ìë™ìœ¼ë¡œ ì°¾ëŠ” ê³¼ì •**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "í¼ì…‰íŠ¸ë¡ : ê°€ì¤‘ì¹˜ë¥¼ ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •\n",
    "ì‹ ê²½ë§: ê°€ì¤‘ì¹˜ë¥¼ ë°ì´í„°ë¡œë¶€í„° í•™ìŠµ\n",
    "\n",
    "í•™ìŠµ = ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”í•˜ëŠ” ê°€ì¤‘ì¹˜ ì°¾ê¸°\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### ì†ì‹¤ í•¨ìˆ˜ (Loss Function)\n",
    "\n",
    "**ì‹ ê²½ë§ ëª¨ë¸ì˜ ì˜ˆì¸¡ ê°’ê³¼ ì‹¤ì œ ê°’ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ëŠ” í•¨ìˆ˜**\n",
    "\n",
    "- ì‹ ê²½ë§ ì„±ëŠ¥ì˜ \"ë‚˜ì¨\"ì„ ë‚˜íƒ€ë‚´ëŠ” ì§€í‘œ\n",
    "- ê°’ì´ ì‘ì„ìˆ˜ë¡ ì¢‹ì€ ëª¨ë¸\n",
    "- í•™ìŠµì˜ ëª©í‘œ: ì†ì‹¤ í•¨ìˆ˜ë¥¼ ìµœì†Œí™”\n",
    "\n",
    "---\n",
    "\n",
    "## 2. ì†ì‹¤ í•¨ìˆ˜ì˜ ì¢…ë¥˜ â­â­â­\n",
    "\n",
    "### 2.1 ì˜¤ì°¨ì œê³±í•© (SSE: Sum of Squares for Error)\n",
    "\n",
    "**ìˆ˜ì‹:**\n",
    "$$E = \\frac{1}{2}\\sum_{k}(y_k - t_k)^2$$\n",
    "\n",
    "- $y_k$: ì‹ ê²½ë§ì˜ ì¶œë ¥ (ì˜ˆì¸¡ê°’)\n",
    "- $t_k$: ì •ë‹µ ë ˆì´ë¸” (ì›-í•« ì¸ì½”ë”©)\n",
    "- $k$: ë°ì´í„°ì˜ ì°¨ì› ìˆ˜\n",
    "\n",
    "**êµ¬í˜„:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sum_squares_error(y, t):\n",
    "    \"\"\"ì˜¤ì°¨ì œê³±í•©\"\"\"\n",
    "    return 0.5 * np.sum((y - t)**2)\n",
    "\n",
    "# ì˜ˆì‹œ\n",
    "# ì •ë‹µ: 2\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])  # ì›-í•« ì¸ì½”ë”©\n",
    "\n",
    "# ì˜ˆì¸¡ 1: '2'ì¼ í™•ë¥ ì´ ê°€ì¥ ë†’ìŒ (ì •ë‹µ)\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(sum_squares_error(y1, t))  # 0.09750000000000003\n",
    "\n",
    "# ì˜ˆì¸¡ 2: '7'ì¼ í™•ë¥ ì´ ê°€ì¥ ë†’ìŒ (ì˜¤ë‹µ)\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print(sum_squares_error(y2, t))  # 0.5975\n",
    "```\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- íšŒê·€ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©\n",
    "- ì˜¤ì°¨ì˜ ì œê³±ì„ ì‚¬ìš© (í° ì˜¤ì°¨ì— ë” í° í˜ë„í‹°)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ (CEE: Cross Entropy Error) â­â­â­\n",
    "\n",
    "**ìˆ˜ì‹:**\n",
    "$$E = -\\sum_{k}t_k\\log y_k$$\n",
    "\n",
    "- ì •ë‹µì¼ ë•Œì˜ ì¶œë ¥ë§Œ ì˜í–¥ (ì›-í•« ì¸ì½”ë”©)\n",
    "- ì •ë‹µì— í•´ë‹¹í•˜ëŠ” ì¶œë ¥ì´ í´ìˆ˜ë¡ ì†ì‹¤ ì‘ìŒ\n",
    "\n",
    "**êµ¬í˜„:**\n",
    "```python\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨\"\"\"\n",
    "    delta = 1e-7  # ì•„ì£¼ ì‘ì€ ê°’\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# ì˜ˆì‹œ\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# ì˜ˆì¸¡ 1: '2'ì¼ í™•ë¥ ì´ ë†’ìŒ (ì •ë‹µ)\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(cross_entropy_error(y1, t))  # 0.510825457099338\n",
    "\n",
    "# ì˜ˆì¸¡ 2: '7'ì¼ í™•ë¥ ì´ ë†’ìŒ (ì˜¤ë‹µ)\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print(cross_entropy_error(y2, t))  # 2.302584092994546\n",
    "```\n",
    "\n",
    "**deltaë¥¼ ë”í•˜ëŠ” ì´ìœ :**\n",
    "```python\n",
    "# np.log(0) = -âˆ (ë§ˆì´ë„ˆìŠ¤ ë¬´í•œëŒ€)\n",
    "# ê³„ì‚° ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ì•„ì£¼ ì‘ì€ ê°’ì„ ë”í•¨\n",
    "\n",
    "y = 0\n",
    "print(np.log(y))        # ì˜¤ë¥˜ ë˜ëŠ” -inf\n",
    "print(np.log(y + 1e-7)) # -16.11809565095832\n",
    "```\n",
    "\n",
    "**íŠ¹ì§•:**\n",
    "- ë¶„ë¥˜ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©\n",
    "- ì •ë‹µ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” ì¶œë ¥ì˜ ë¡œê·¸ë¥¼ ê³„ì‚°\n",
    "- í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •\n",
    "\n",
    "---\n",
    "\n",
    "## 3. ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ â­â­â­\n",
    "\n",
    "### ë°°ì¹˜ìš© ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "**ìˆ˜ì‹:**\n",
    "$$E = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log y_{nk}$$\n",
    "\n",
    "- $N$: ë°°ì¹˜ í¬ê¸°\n",
    "- ë°ì´í„° 1ê°œë‹¹ í‰ê·  ì†ì‹¤ í•¨ìˆ˜ ê³„ì‚°\n",
    "\n",
    "### ë¯¸ë‹ˆë°°ì¹˜ë€?\n",
    "\n",
    "**í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•˜ì—¬ í•™ìŠµí•˜ëŠ” ë°©ë²•**\n",
    "\n",
    "```python\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(\n",
    "    normalize=True, \n",
    "    one_hot_label=True\n",
    ")\n",
    "\n",
    "print(x_train.shape)  # (60000, 784)\n",
    "print(t_train.shape)  # (60000, 10)\n",
    "\n",
    "# ë¯¸ë‹ˆë°°ì¹˜ ì„ íƒ\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "print(x_batch.shape)  # (10, 784)\n",
    "print(t_batch.shape)  # (10, 10)\n",
    "```\n",
    "\n",
    "### ë°°ì¹˜ìš© êµì°¨ ì—”íŠ¸ë¡œí”¼ êµ¬í˜„\n",
    "\n",
    "```python\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"\n",
    "    ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ì§€ì›í•˜ëŠ” êµì°¨ ì—”íŠ¸ë¡œí”¼\n",
    "    \n",
    "    y: ì‹ ê²½ë§ ì¶œë ¥\n",
    "    t: ì •ë‹µ ë ˆì´ë¸”\n",
    "    \"\"\"\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    # ë°°ì¹˜ í¬ê¸°\n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    # ì›-í•« ì¸ì½”ë”©ì¸ ê²½ìš°\n",
    "    if t.size == y.size:\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "    \n",
    "    # ì •ë‹µ ë ˆì´ë¸”ì´ ìˆ«ìì¸ ê²½ìš°\n",
    "    else:\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "```\n",
    "\n",
    "**ì‚¬ìš© ì˜ˆ:**\n",
    "```python\n",
    "# ì›-í•« ì¸ì½”ë”©\n",
    "t = np.array([[0, 0, 1, 0],\n",
    "              [0, 1, 0, 0]])\n",
    "y = np.array([[0.1, 0.2, 0.6, 0.1],\n",
    "              [0.2, 0.5, 0.2, 0.1]])\n",
    "print(cross_entropy_error(y, t))\n",
    "\n",
    "# ë ˆì´ë¸” í˜•ì‹\n",
    "t = np.array([2, 1])  # ì •ë‹µ ì¸ë±ìŠ¤\n",
    "y = np.array([[0.1, 0.2, 0.6, 0.1],\n",
    "              [0.2, 0.5, 0.2, 0.1]])\n",
    "print(cross_entropy_error(y, t))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. ì™œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ”ê°€? â­â­â­\n",
    "\n",
    "### ì •í™•ë„ë¥¼ ì§€í‘œë¡œ ì‚¬ìš©í•˜ë©´ ì•ˆ ë˜ëŠ” ì´ìœ \n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "ì •í™•ë„ ë¬¸ì œì :\n",
    "1. ë¶ˆì—°ì†ì  (0% â†’ 100%)\n",
    "2. ë¯¸ë¶„ì´ ëŒ€ë¶€ë¶„ 0\n",
    "3. ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ì„¸í•œ ë³€í™”ë¥¼ ë°˜ì˜ ëª»í•¨\n",
    "\n",
    "ì˜ˆ:\n",
    "ê°€ì¤‘ì¹˜ë¥¼ ì¡°ê¸ˆ ë³€ê²½ â†’ ì •í™•ë„ ë³€í™” ì—†ìŒ (33% â†’ 33%)\n",
    "â†’ ê¸°ìš¸ê¸°ê°€ 0 â†’ í•™ìŠµ ë¶ˆê°€ëŠ¥\n",
    "\n",
    "ì†ì‹¤ í•¨ìˆ˜:\n",
    "1. ì—°ì†ì \n",
    "2. ë¯¸ë¶„ ê°€ëŠ¥\n",
    "3. ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ì„¸í•œ ë³€í™” ë°˜ì˜\n",
    "â†’ í•™ìŠµ ê°€ëŠ¥!\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**ì˜ˆì‹œ:**\n",
    "```python\n",
    "# 100ê°œ ë°ì´í„° ì¤‘ 32ê°œ ë§ì¶¤ â†’ ì •í™•ë„ 32%\n",
    "# ê°€ì¤‘ì¹˜ ì¡°ê¸ˆ ë³€ê²½\n",
    "# 100ê°œ ë°ì´í„° ì¤‘ 32ê°œ ë§ì¶¤ â†’ ì •í™•ë„ 32% (ë³€í™” ì—†ìŒ!)\n",
    "\n",
    "# í•˜ì§€ë§Œ ì†ì‹¤ í•¨ìˆ˜ëŠ”:\n",
    "# ì†ì‹¤: 2.3\n",
    "# ê°€ì¤‘ì¹˜ ì¡°ê¸ˆ ë³€ê²½\n",
    "# ì†ì‹¤: 2.29 (ê°ì†Œ!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. ìˆ˜ì¹˜ ë¯¸ë¶„ â­â­\n",
    "\n",
    "### ë¯¸ë¶„ì˜ ì •ì˜\n",
    "\n",
    "**í•´ì„ì  ë¯¸ë¶„:**\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**ë¬¸ì œì :** ë°˜ì˜¬ë¦¼ ì˜¤ì°¨ (rounding error)\n",
    "\n",
    "### ìˆ˜ì¹˜ ë¯¸ë¶„ (Numerical Differentiation)\n",
    "\n",
    "**ì¤‘ì‹¬ ì°¨ë¶„ / ì¤‘ì•™ ì°¨ë¶„:**\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x-h)}{2h}$$\n",
    "\n",
    "**êµ¬í˜„:**\n",
    "```python\n",
    "def numerical_diff(f, x):\n",
    "    \"\"\"ìˆ˜ì¹˜ ë¯¸ë¶„\"\"\"\n",
    "    h = 1e-4  # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "# ì˜ˆì‹œ: f(x) = x^2\n",
    "def function_1(x):\n",
    "    return x**2\n",
    "\n",
    "# x=5ì—ì„œì˜ ë¯¸ë¶„\n",
    "print(numerical_diff(function_1, 5))   # 10.000000000139778\n",
    "print(numerical_diff(function_1, 10))  # 20.00000000279556\n",
    "\n",
    "# í•´ì„ì  ë¯¸ë¶„: f'(x) = 2x\n",
    "# x=5 â†’ 10\n",
    "# x=10 â†’ 20\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. í¸ë¯¸ë¶„ â­â­\n",
    "\n",
    "### ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ë¯¸ë¶„\n",
    "\n",
    "**í•¨ìˆ˜:**\n",
    "$$f(x_0, x_1) = x_0^2 + x_1^2$$\n",
    "\n",
    "**êµ¬í˜„:**\n",
    "```python\n",
    "def function_2(x):\n",
    "    \"\"\"f(x0, x1) = x0^2 + x1^2\"\"\"\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # ë˜ëŠ” return np.sum(x**2)\n",
    "```\n",
    "\n",
    "### í¸ë¯¸ë¶„ ê³„ì‚°\n",
    "\n",
    "**ë¬¸ì œ:** $x_0 = 3, x_1 = 4$ì¼ ë•Œ, $\\frac{\\partial f}{\\partial x_0}$ë¥¼ êµ¬í•˜ë¼.\n",
    "\n",
    "```python\n",
    "# x1ì„ 4ë¡œ ê³ ì •\n",
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4.0**2\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))  # 6.00000000000378\n",
    "\n",
    "# í•´ì„ì  ë¯¸ë¶„: âˆ‚f/âˆ‚x0 = 2x0\n",
    "# x0=3 â†’ 2*3 = 6\n",
    "```\n",
    "\n",
    "**ë¬¸ì œ:** $x_0 = 3, x_1 = 4$ì¼ ë•Œ, $\\frac{\\partial f}{\\partial x_1}$ë¥¼ êµ¬í•˜ë¼.\n",
    "\n",
    "```python\n",
    "# x0ì„ 3ìœ¼ë¡œ ê³ ì •\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2 + x1**2\n",
    "\n",
    "print(numerical_diff(function_tmp2, 4.0))  # 7.999999999999119\n",
    "\n",
    "# í•´ì„ì  ë¯¸ë¶„: âˆ‚f/âˆ‚x1 = 2x1\n",
    "# x1=4 â†’ 2*4 = 8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. ê¸°ìš¸ê¸° (Gradient) â­â­â­\n",
    "\n",
    "### ê¸°ìš¸ê¸°ë€?\n",
    "\n",
    "**ëª¨ë“  ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„ì„ ë²¡í„°ë¡œ ì •ë¦¬í•œ ê²ƒ**\n",
    "\n",
    "$$\\nabla f = \\left(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}\\right)$$\n",
    "\n",
    "### êµ¬í˜„\n",
    "\n",
    "```python\n",
    "def numerical_gradient(f, x):\n",
    "    \"\"\"ê¸°ìš¸ê¸° ê³„ì‚°\"\"\"\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)  # xì™€ ê°™ì€ í˜•ìƒì˜ ë°°ì—´\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) ê³„ì‚°\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) ê³„ì‚°\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        # ì¤‘ì‹¬ ì°¨ë¶„\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        # ê°’ ë³µì›\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# (3, 4)ì—ì„œì˜ ê¸°ìš¸ê¸°\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "# [6. 8.]\n",
    "\n",
    "# (0, 2)ì—ì„œì˜ ê¸°ìš¸ê¸°\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "# [0. 4.]\n",
    "\n",
    "# (3, 0)ì—ì„œì˜ ê¸°ìš¸ê¸°\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))\n",
    "# [6. 0.]\n",
    "```\n",
    "\n",
    "### ê¸°ìš¸ê¸°ì˜ ì˜ë¯¸\n",
    "\n",
    "**ê¸°ìš¸ê¸°ê°€ ê°€ë¦¬í‚¤ëŠ” ë°©í–¥ = ê° ì§€ì ì—ì„œ í•¨ìˆ˜ì˜ ì¶œë ¥ ê°’ì„ ê°€ì¥ í¬ê²Œ ì¤„ì´ëŠ” ë°©í–¥**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "f(x0, x1) = x0^2 + x1^2\n",
    "\n",
    "(3, 4)ì—ì„œ ê¸°ìš¸ê¸°: [6, 8]\n",
    "â†’ x0ë¥¼ ìŒì˜ ë°©í–¥ìœ¼ë¡œ, x1ì„ ìŒì˜ ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë©´\n",
    "  í•¨ìˆ˜ ê°’ì´ ê°ì†Œ\n",
    "\n",
    "ì›ì  (0, 0)ì—ì„œ ê¸°ìš¸ê¸°: [0, 0]\n",
    "â†’ ìµœì†Ÿê°’ ì§€ì  (ê·¹ì†Ÿê°’)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. ê²½ì‚¬í•˜ê°•ë²• (Gradient Descent) â­â­â­\n",
    "\n",
    "### ê°œë…\n",
    "\n",
    "**ê¸°ìš¸ê¸°ë¥¼ ì´ìš©í•˜ì—¬ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ì°¾ëŠ” ë°©ë²•**\n",
    "\n",
    "$$x_{new} = x_{old} - \\eta \\nabla f(x_{old})$$\n",
    "\n",
    "- $\\eta$: í•™ìŠµë¥  (learning rate)\n",
    "- $\\nabla f$: ê¸°ìš¸ê¸°\n",
    "\n",
    "### êµ¬í˜„\n",
    "\n",
    "```python\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    \"\"\"\n",
    "    ê²½ì‚¬í•˜ê°•ë²•\n",
    "    \n",
    "    f: ìµœì í™”í•  í•¨ìˆ˜\n",
    "    init_x: ì´ˆê¹ƒê°’\n",
    "    lr: í•™ìŠµë¥  (learning rate)\n",
    "    step_num: ë°˜ë³µ íšŸìˆ˜\n",
    "    \"\"\"\n",
    "    x = init_x\n",
    "    x_history = []  # ì´ë™ ê²½ë¡œ ê¸°ë¡\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        \n",
    "        # ê¸°ìš¸ê¸° ê³„ì‚°\n",
    "        grad = numerical_gradient(f, x)\n",
    "        \n",
    "        # ê°±ì‹ \n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x, np.array(x_history)\n",
    "\n",
    "# í…ŒìŠ¤íŠ¸\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# ì´ˆê¹ƒê°’ (-3, 4)\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "# í•™ìŠµë¥  0.1\n",
    "x, history = gradient_descent(function_2, init_x, lr=0.1, step_num=100)\n",
    "print(x)  # [-6.11110793e-10  8.14814391e-10] â‰ˆ (0, 0)\n",
    "```\n",
    "\n",
    "### í•™ìŠµë¥ ì˜ ì¤‘ìš”ì„± â­â­\n",
    "\n",
    "```python\n",
    "# í•™ìŠµë¥ ì´ ë„ˆë¬´ í° ê²½ìš°\n",
    "x, _ = gradient_descent(function_2, init_x, lr=10.0, step_num=100)\n",
    "print(x)  # [-2.58983747e+13 -1.29524862e+12] â†’ ë°œì‚°!\n",
    "\n",
    "# í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì€ ê²½ìš°\n",
    "x, _ = gradient_descent(function_2, init_x, lr=1e-10, step_num=100)\n",
    "print(x)  # [-2.99999994  3.99999992] â†’ ê±°ì˜ ì´ë™ ì•ˆ í•¨!\n",
    "\n",
    "# ì ì ˆí•œ í•™ìŠµë¥ \n",
    "x, _ = gradient_descent(function_2, init_x, lr=0.1, step_num=100)\n",
    "print(x)  # [0. 0.] â†’ ìµœì†Ÿê°’ ë„ë‹¬!\n",
    "```\n",
    "\n",
    "**í•µì‹¬:**\n",
    "- í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ë©´ â†’ ë°œì‚°\n",
    "- í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ â†’ í•™ìŠµ ì•ˆ ë¨\n",
    "- ì ì ˆí•œ í•™ìŠµë¥  ì„ íƒì´ ì¤‘ìš” (í•˜ì´í¼íŒŒë¼ë¯¸í„°)\n",
    "\n",
    "---\n",
    "\n",
    "## 9. ì‹ ê²½ë§ì˜ ê¸°ìš¸ê¸° â­â­â­\n",
    "\n",
    "### SimpleNet êµ¬í˜„\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™” (2Ã—3 í–‰ë ¬)\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"ì˜ˆì¸¡\"\"\"\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"ì†ì‹¤ ê³„ì‚°\"\"\"\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "net = SimpleNet()\n",
    "print(net.W)\n",
    "# [[ 0.47355232  0.9977393   0.84668094]\n",
    "#  [ 0.85557411  0.03563661  0.69422093]]\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "# [1.05414809 0.63071653 1.13269074]\n",
    "\n",
    "print(np.argmax(p))  # 2 (ìµœëŒ“ê°’ ì¸ë±ìŠ¤)\n",
    "\n",
    "t = np.array([0, 0, 1])  # ì •ë‹µ ë ˆì´ë¸”\n",
    "print(net.loss(x, t))    # 0.92806853663411326\n",
    "```\n",
    "\n",
    "### ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ê¸°ìš¸ê¸°\n",
    "\n",
    "```python\n",
    "def f(W):\n",
    "    \"\"\"ì†ì‹¤ í•¨ìˆ˜ (ê°€ì¤‘ì¹˜ë¥¼ ì¸ìˆ˜ë¡œ)\"\"\"\n",
    "    return net.loss(x, t)\n",
    "\n",
    "# ê¸°ìš¸ê¸° ê³„ì‚°\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n",
    "# [[ 0.21924763  0.14356247 -0.36281009]\n",
    "#  [ 0.32887144  0.2153437  -0.54421514]]\n",
    "```\n",
    "\n",
    "### ê¸°ìš¸ê¸°ì˜ ì˜ë¯¸\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "dW[0, 0] â‰ˆ 0.22\n",
    "â†’ w11ì„ hë§Œí¼ ì¦ê°€ì‹œí‚¤ë©´ ì†ì‹¤ì´ 0.22hë§Œí¼ ì¦ê°€\n",
    "\n",
    "dW[1, 2] â‰ˆ -0.54\n",
    "â†’ w23ì„ hë§Œí¼ ì¦ê°€ì‹œí‚¤ë©´ ì†ì‹¤ì´ 0.54hë§Œí¼ ê°ì†Œ\n",
    "\n",
    "ë”°ë¼ì„œ:\n",
    "- ì–‘ìˆ˜ ê¸°ìš¸ê¸°: ê°€ì¤‘ì¹˜ë¥¼ ê°ì†Œì‹œì¼œì•¼ í•¨\n",
    "- ìŒìˆ˜ ê¸°ìš¸ê¸°: ê°€ì¤‘ì¹˜ë¥¼ ì¦ê°€ì‹œì¼œì•¼ í•¨\n",
    "- ì ˆëŒ“ê°’ì´ í´ìˆ˜ë¡: ì†ì‹¤ì— ë” í° ì˜í–¥\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„ â­â­â­\n",
    "\n",
    "### ì‹ ê²½ë§ í•™ìŠµ 4ë‹¨ê³„\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "1ë‹¨ê³„: ë¯¸ë‹ˆë°°ì¹˜\n",
    "   - í›ˆë ¨ ë°ì´í„° ì¤‘ ì¼ë¶€ë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒ\n",
    "\n",
    "2ë‹¨ê³„: ê¸°ìš¸ê¸° ì‚°ì¶œ\n",
    "   - ê° ê°€ì¤‘ì¹˜ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ì†ì‹¤ í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸° ê³„ì‚°\n",
    "\n",
    "3ë‹¨ê³„: ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \n",
    "   - ê°€ì¤‘ì¹˜ë¥¼ ê¸°ìš¸ê¸° ë°©í–¥ìœ¼ë¡œ ì¡°ê¸ˆ ê°±ì‹ \n",
    "\n",
    "4ë‹¨ê³„: ë°˜ë³µ\n",
    "   - 1~3ë‹¨ê³„ë¥¼ ë°˜ë³µ\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### 2ì¸µ ì‹ ê²½ë§ í´ë˜ìŠ¤\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 weight_init_std=0.01):\n",
    "        \"\"\"\n",
    "        ì´ˆê¸°í™”\n",
    "        \n",
    "        input_size: ì…ë ¥ì¸µ ë‰´ëŸ° ìˆ˜\n",
    "        hidden_size: ì€ë‹‰ì¸µ ë‰´ëŸ° ìˆ˜\n",
    "        output_size: ì¶œë ¥ì¸µ ë‰´ëŸ° ìˆ˜\n",
    "        \"\"\"\n",
    "        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                           np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                           np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"ì˜ˆì¸¡ (ìˆœì „íŒŒ)\"\"\"\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"ì†ì‹¤ í•¨ìˆ˜ ê°’\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"ì •í™•ë„\"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"ê¸°ìš¸ê¸° ê³„ì‚° (ìˆ˜ì¹˜ ë¯¸ë¶„)\"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "```\n",
    "\n",
    "### í•™ìŠµ êµ¬í˜„\n",
    "\n",
    "```python\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# ë°ì´í„° ë¡œë“œ\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(\n",
    "    normalize=True, \n",
    "    one_hot_label=True\n",
    ")\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„°\n",
    "iters_num = 10000  # ë°˜ë³µ íšŸìˆ˜\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1ì—í­ë‹¹ ë°˜ë³µ ìˆ˜\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# ì‹ ê²½ë§ ìƒì„±\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # ë¯¸ë‹ˆë°°ì¹˜ íšë“\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # ê¸°ìš¸ê¸° ê³„ì‚°\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # ë§¤ê°œë³€ìˆ˜ ê°±ì‹ \n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # í•™ìŠµ ê²½ê³¼ ê¸°ë¡\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1ì—í­ë‹¹ ì •í™•ë„ ê³„ì‚°\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, x_test)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"train acc, test acc | {train_acc}, {test_acc}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. ìš©ì–´ ì •ë¦¬ â­â­â­\n",
    "\n",
    "### ì—í­ (Epoch)\n",
    "\n",
    "**ëª¨ë“  í›ˆë ¨ ë°ì´í„°ë¥¼ í•œ ë²ˆì”© í•™ìŠµí•œ ê²ƒ**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "í›ˆë ¨ ë°ì´í„°: 60,000ê°œ\n",
    "ë¯¸ë‹ˆë°°ì¹˜: 100ê°œ\n",
    "\n",
    "1 ì—í­ = 60,000 / 100 = 600íšŒ ë°˜ë³µ\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²• (SGD: Stochastic Gradient Descent)\n",
    "\n",
    "**ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œ ë¯¸ë‹ˆë°°ì¹˜ì— ëŒ€í•´ ê²½ì‚¬í•˜ê°•ë²•ì„ ìˆ˜í–‰**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "ì „ì²´ ë°ì´í„°: ë¹„ìš©ì´ ë„ˆë¬´ í¼\n",
    "ë¯¸ë‹ˆë°°ì¹˜: ë¬´ì‘ìœ„ ì„ íƒ â†’ í™•ë¥ ì  (Stochastic)\n",
    "\n",
    "ì¥ì :\n",
    "- ë¹ ë¥¸ í•™ìŠµ\n",
    "- ë©”ëª¨ë¦¬ íš¨ìœ¨ì \n",
    "- ì§€ì—­ ìµœì†Ÿê°’ íƒˆì¶œ ê°€ëŠ¥\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. ì‹œí—˜ ì˜ˆìƒ ë¬¸ì œ\n",
    "\n",
    "### ë¬¸ì œ 1 (10ì )\n",
    "```\n",
    "ì˜¤ì°¨ì œê³±í•©(SSE)ê³¼ êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨(CEE)ì˜ ìˆ˜ì‹ì„ ì“°ê³ ,\n",
    "ê°ê° ì–´ë–¤ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ”ì§€ ì„¤ëª…í•˜ì‹œì˜¤.\n",
    "```\n",
    "\n",
    "**ë‹µì•ˆ:**\n",
    "```\n",
    "1. ì˜¤ì°¨ì œê³±í•© (SSE):\n",
    "   E = (1/2)Î£(yk - tk)Â²\n",
    "   - íšŒê·€ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©\n",
    "   - ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì˜ ì°¨ì´ë¥¼ ì œê³±í•˜ì—¬ í•©ì‚°\n",
    "\n",
    "2. êµì°¨ ì—”íŠ¸ë¡œí”¼ ì˜¤ì°¨ (CEE):\n",
    "   E = -Î£tk log(yk)\n",
    "   - ë¶„ë¥˜ ë¬¸ì œì— ì£¼ë¡œ ì‚¬ìš©\n",
    "   - ì •ë‹µ ë ˆì´ë¸”ì— í•´ë‹¹í•˜ëŠ” ì¶œë ¥ì˜ ë¡œê·¸ë¥¼ ê³„ì‚°\n",
    "   - í™•ë¥  ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ë¬¸ì œ 2 (10ì )\n",
    "```\n",
    "ì‹ ê²½ë§ í•™ìŠµì—ì„œ ì •í™•ë„ê°€ ì•„ë‹Œ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” \n",
    "ì´ìœ ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n",
    "```\n",
    "\n",
    "**ë‹µì•ˆ:**\n",
    "```\n",
    "ì •í™•ë„ëŠ” ë¶ˆì—°ì†ì ì´ê³  ëŒ€ë¶€ë¶„ì˜ ì§€ì ì—ì„œ ë¯¸ë¶„ê°’ì´ 0ì´ ë˜ì–´\n",
    "ë§¤ê°œë³€ìˆ˜ì˜ ë¯¸ì„¸í•œ ë³€í™”ë¥¼ ë°˜ì˜í•˜ì§€ ëª»í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n",
    "\n",
    "ì˜ˆë¥¼ ë“¤ì–´, 100ê°œ ë°ì´í„° ì¤‘ 32ê°œë¥¼ ë§ì¶”ë©´ ì •í™•ë„ëŠ” 32%ì´ê³ ,\n",
    "ê°€ì¤‘ì¹˜ë¥¼ ì¡°ê¸ˆ ë³€ê²½í•´ë„ ì—¬ì „íˆ 32ê°œë¥¼ ë§ì¶”ë©´ ì •í™•ë„ëŠ” 32%ë¡œ\n",
    "ë³€í™”ê°€ ì—†ë‹¤. ì´ ê²½ìš° ê¸°ìš¸ê¸°ê°€ 0ì´ ë˜ì–´ í•™ìŠµì´ ë¶ˆê°€ëŠ¥í•˜ë‹¤.\n",
    "\n",
    "ë°˜ë©´ ì†ì‹¤ í•¨ìˆ˜ëŠ” ì—°ì†ì ì´ê³  ë¯¸ë¶„ ê°€ëŠ¥í•˜ì—¬ ë§¤ê°œë³€ìˆ˜ì˜\n",
    "ë¯¸ì„¸í•œ ë³€í™”ë„ ë°˜ì˜í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ í•™ìŠµì´ ê°€ëŠ¥í•˜ë‹¤.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ë¬¸ì œ 3 (15ì )\n",
    "```\n",
    "ë‹¤ìŒ í•¨ìˆ˜ì— ëŒ€í•´ (3, 4) ì§€ì ì—ì„œì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•˜ì‹œì˜¤.\n",
    "\n",
    "f(x0, x1) = x0Â² + x1Â²\n",
    "\n",
    "ê·¸ë¦¬ê³  ê¸°ìš¸ê¸°ì˜ ì˜ë¯¸ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n",
    "```\n",
    "\n",
    "**ë‹µì•ˆ:**\n",
    "```python\n",
    "# í¸ë¯¸ë¶„\n",
    "âˆ‚f/âˆ‚x0 = 2x0 = 2(3) = 6\n",
    "âˆ‚f/âˆ‚x1 = 2x1 = 2(4) = 8\n",
    "\n",
    "# ê¸°ìš¸ê¸°\n",
    "âˆ‡f = (6, 8)\n",
    "\n",
    "# ì˜ë¯¸:\n",
    "ê¸°ìš¸ê¸° (6, 8)ì€ (3, 4) ì§€ì ì—ì„œ í•¨ìˆ˜ fì˜ ê°’ì´\n",
    "ê°€ì¥ ê°€íŒŒë¥´ê²Œ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ë‚˜íƒ€ë‚¸ë‹¤.\n",
    "\n",
    "ë”°ë¼ì„œ í•¨ìˆ˜ì˜ ìµœì†Ÿê°’ì„ ì°¾ìœ¼ë ¤ë©´ ê¸°ìš¸ê¸°ì˜ ë°˜ëŒ€ ë°©í–¥ì¸\n",
    "(-6, -8) ë°©í–¥ìœ¼ë¡œ ì´ë™í•´ì•¼ í•œë‹¤.\n",
    "\n",
    "ì´ê²ƒì´ ê²½ì‚¬í•˜ê°•ë²•ì˜ ì›ë¦¬ì´ë‹¤.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ë¬¸ì œ 4 (15ì )\n",
    "```\n",
    "ê²½ì‚¬í•˜ê°•ë²•ì˜ ê°±ì‹  ìˆ˜ì‹ì„ ì“°ê³ , í•™ìŠµë¥ ì´ ë„ˆë¬´ í¬ê±°ë‚˜\n",
    "ë„ˆë¬´ ì‘ì„ ë•Œ ë°œìƒí•˜ëŠ” ë¬¸ì œë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n",
    "```\n",
    "\n",
    "**ë‹µì•ˆ:**\n",
    "```\n",
    "ê°±ì‹  ìˆ˜ì‹:\n",
    "x_new = x_old - Î·âˆ‡f(x_old)\n",
    "\n",
    "ì—¬ê¸°ì„œ Î·ëŠ” í•™ìŠµë¥ (learning rate)\n",
    "\n",
    "í•™ìŠµë¥ ì´ ë„ˆë¬´ í° ê²½ìš°:\n",
    "- ê°±ì‹  í­ì´ ë„ˆë¬´ ì»¤ì„œ ìµœì†Ÿê°’ì„ ì§€ë‚˜ì³ ë²„ë¦¼\n",
    "- ë°œì‚°(divergence)í•  ìˆ˜ ìˆìŒ\n",
    "- ì˜ˆ: Î· = 10.0 â†’ ê°’ì´ ë¬´í•œëŒ€ë¡œ ì¦ê°€\n",
    "\n",
    "í•™ìŠµë¥ ì´ ë„ˆë¬´ ì‘ì€ ê²½ìš°:\n",
    "- ê°±ì‹  í­ì´ ë„ˆë¬´ ì‘ì•„ì„œ í•™ìŠµì´ ëŠë¦¼\n",
    "- ìµœì†Ÿê°’ì— ë„ë‹¬í•˜ì§€ ëª»í•¨\n",
    "- ì˜ˆ: Î· = 1e-10 â†’ ê±°ì˜ ì´ë™í•˜ì§€ ì•ŠìŒ\n",
    "\n",
    "ë”°ë¼ì„œ ì ì ˆí•œ í•™ìŠµë¥ ì„ ì„ íƒí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤.\n",
    "(ì¼ë°˜ì ìœ¼ë¡œ 0.01, 0.001 ë“± ì‚¬ìš©)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ë¬¸ì œ 5 (10ì )\n",
    "```\n",
    "ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµì˜ ì¥ì  3ê°€ì§€ë¥¼ ì„¤ëª…í•˜ì‹œì˜¤.\n",
    "```\n",
    "\n",
    "**ë‹µì•ˆ:**\n",
    "```\n",
    "1. ê³„ì‚° íš¨ìœ¨ì„±\n",
    "   - ì „ì²´ ë°ì´í„°ë¥¼ í•œ ë²ˆì— ì²˜ë¦¬í•˜ëŠ” ê²ƒë³´ë‹¤ ë¹ ë¦„\n",
    "   - í–‰ë ¬ ì—°ì‚°ì˜ ìµœì í™” í™œìš©\n",
    "\n",
    "2. ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±\n",
    "   - ì „ì²´ ë°ì´í„°ë¥¼ ë©”ëª¨ë¦¬ì— ì˜¬ë¦´ í•„ìš” ì—†ìŒ\n",
    "   - GPU ë©”ëª¨ë¦¬ ì œì•½ ê·¹ë³µ\n",
    "\n",
    "3. ì¼ë°˜í™” ì„±ëŠ¥ í–¥ìƒ\n",
    "   - ë¬´ì‘ìœ„ ì„ íƒìœ¼ë¡œ ì¸í•œ ë…¸ì´ì¦ˆê°€ \n",
    "     ì§€ì—­ ìµœì†Ÿê°’ íƒˆì¶œì— ë„ì›€\n",
    "   - ê³¼ì í•©(overfitting) ë°©ì§€\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 13. í•µì‹¬ ê°œë… ì •ë¦¬\n",
    "\n",
    "### âœ… ë°˜ë“œì‹œ ì•Œì•„ì•¼ í•  ê²ƒ\n",
    "\n",
    "1. **ì†ì‹¤ í•¨ìˆ˜**\n",
    "   - SSE: íšŒê·€ ë¬¸ì œ\n",
    "   - CEE: ë¶„ë¥˜ ë¬¸ì œ\n",
    "   - ì›-í•« ì¸ì½”ë”©\n",
    "\n",
    "2. **ë¯¸ë‹ˆë°°ì¹˜**\n",
    "   - ì¼ë¶€ ë°ì´í„°ë¡œ í•™ìŠµ\n",
    "   - í‰ê·  ì†ì‹¤ ê³„ì‚°\n",
    "   - í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•\n",
    "\n",
    "3. **ìˆ˜ì¹˜ ë¯¸ë¶„**\n",
    "   - ì¤‘ì‹¬ ì°¨ë¶„ ì‚¬ìš©\n",
    "   - h = 1e-4\n",
    "   - ê·¼ì‚¬ê°’ ê³„ì‚°\n",
    "\n",
    "4. **ê¸°ìš¸ê¸°**\n",
    "   - ëª¨ë“  ë³€ìˆ˜ì˜ í¸ë¯¸ë¶„\n",
    "   - í•¨ìˆ˜ê°€ ê°€ì¥ í¬ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥\n",
    "   - ê²½ì‚¬í•˜ê°•ë²•ì— ì‚¬ìš©\n",
    "\n",
    "5. **ê²½ì‚¬í•˜ê°•ë²•**\n",
    "   - x_new = x_old - Î·âˆ‡f\n",
    "   - í•™ìŠµë¥ ì˜ ì¤‘ìš”ì„±\n",
    "   - ë°˜ë³µì  ê°±ì‹ \n",
    "\n",
    "6. **í•™ìŠµ ì•Œê³ ë¦¬ì¦˜**\n",
    "   - ë¯¸ë‹ˆë°°ì¹˜ â†’ ê¸°ìš¸ê¸° â†’ ê°±ì‹  â†’ ë°˜ë³µ\n",
    "   - ì—í­: ì „ì²´ ë°ì´í„° 1íšŒ í•™ìŠµ\n",
    "   - SGD: í™•ë¥ ì  ê²½ì‚¬í•˜ê°•ë²•\n",
    "\n",
    "---\n",
    "\n",
    "## 14. ë¹ ë¥¸ ë³µìŠµ ì²´í¬ë¦¬ìŠ¤íŠ¸\n",
    "\n",
    "- [ ] ì†ì‹¤ í•¨ìˆ˜ì˜ ê°œë…ê³¼ ì¢…ë¥˜\n",
    "- [ ] SSEì™€ CEE ìˆ˜ì‹\n",
    "- [ ] ë¯¸ë‹ˆë°°ì¹˜ í•™ìŠµ\n",
    "- [ ] ì •í™•ë„ ëŒ€ì‹  ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì“°ëŠ” ì´ìœ \n",
    "- [ ] ìˆ˜ì¹˜ ë¯¸ë¶„ (ì¤‘ì‹¬ ì°¨ë¶„)\n",
    "- [ ] í¸ë¯¸ë¶„ì˜ ê°œë…\n",
    "- [ ] ê¸°ìš¸ê¸°ì˜ ì˜ë¯¸\n",
    "- [ ] ê²½ì‚¬í•˜ê°•ë²• ìˆ˜ì‹\n",
    "- [ ] í•™ìŠµë¥ ì˜ ì—­í• \n",
    "- [ ] ì‹ ê²½ë§ í•™ìŠµ 4ë‹¨ê³„\n",
    "- [ ] ì—í­ê³¼ SGD ê°œë…\n",
    "\n",
    "---\n",
    "\n",
    "## 15. ì½”ë“œ í…œí”Œë¦¿ (ì•”ê¸°ìš©)\n",
    "\n",
    "### ì†ì‹¤ í•¨ìˆ˜\n",
    "\n",
    "```python\n",
    "# SSE\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)\n",
    "\n",
    "# CEE\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "```\n",
    "\n",
    "### ìˆ˜ì¹˜ ë¯¸ë¶„\n",
    "\n",
    "```python\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "```\n",
    "\n",
    "### ê¸°ìš¸ê¸°\n",
    "\n",
    "```python\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad\n",
    "```\n",
    "\n",
    "### ê²½ì‚¬í•˜ê°•ë²•\n",
    "\n",
    "```python\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "```\n",
    "\n",
    "**ì‹œí—˜ í™”ì´íŒ…! ğŸ¯**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777564c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

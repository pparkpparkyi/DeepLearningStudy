{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13ad9b45",
   "metadata": {},
   "source": [
    "# 신경망 학습\n",
    "**학습** : 훈련 데이터로부터 가중치 매개변수의 최적값   \n",
    "[손실함수] : 신경망 모델의 예측 값과 실제 값 간의 차이를 측정하는 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db6a320",
   "metadata": {},
   "source": [
    "## 1. 오차 제곱 합\n",
    "sum of squares for error (SSE)\n",
    "* 원핫 인코딩을 함.\n",
    "\n",
    "$$ E = \\frac{1}{2} \\displaystyle\\sum_k(y_k-t_k)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e85ddd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def sum_squares_error(y,t):\n",
    "    return 0.5*np.sum((y-t)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d56d6",
   "metadata": {},
   "source": [
    "## 2. Cross Entropy Error(CEE)   \n",
    "교차 엔트로피 오차\n",
    "\n",
    "$$E = -\\displaystyle\\sum_k\\;t_k\\;\\text{log}y_k$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39efef0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def cross_entropy_error(y,t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t*np.log(y+delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a01d1b82",
   "metadata": {},
   "source": [
    "delta를 더하는 이유는 np.log()함수엥 0을 입력하면 마이너스 무한대.\n",
    "따라서 아주 작은 값을 더해서 0이 될 수 없게 했음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dfd58c",
   "metadata": {},
   "source": [
    "## 3. 미니배치\n",
    "$$ E= -\\frac {1}{N} \\displaystyle\\sum_n\\displaystyle\\sum_k\\; t_{nk}\\; \\text{log}\\;y_{nk} $$\n",
    "\n",
    "일부만 골라서 학습을 수행하는것.  \n",
    "**미니배치**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53393da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'..'))\n",
    "import numpy as np\n",
    "from dataset.mnist import loasd_mnist\n",
    "\n",
    "(x_train,t_train),(x_test, t_test) = load_mnist(normalize=True, one_hot_label = True)\n",
    "\n",
    "print(x_train.shape) # (60000,784)\n",
    "print(t_train.shape) # (60000,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e7f393",
   "metadata": {},
   "source": [
    "호출할 때 one_hot_label=True로 지정하여 원핫인코딩으로,   \n",
    "앞의 코드에서 MNIST 데이터를 읽은 결과,    \n",
    "훈련데이터는 60,000개고 입력 데이터는 784열 (원래는 28x28)인 이미지 데이터임을 할 수 있다.     \n",
    "정답레이블은 10줄짜리 데이터임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3fbe8",
   "metadata": {},
   "source": [
    "### 왜 손실 함수를 설정하는가?\n",
    "신경망을 학습할 때 정확도를 지표로 삼아서는 안 된다.  \n",
    "정확도를 지표로 하면 매개변수의 미분이 대부분의 장소에서 0이 되기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a1175",
   "metadata": {},
   "source": [
    "## 4. 미분\n",
    "해석적미분   \n",
    "$\\frac{df(x)}{dx}=\\displaystyle\\lim_{h\\rarr\\infin}{\\frac{f(x+h)-f(x)}{h}}$\n",
    "-> 반올림 오차 문제 일으킴   \n",
    "수치미분 numerical differentiation   \n",
    "**중심차분** **중앙차분**으로 해결   \n",
    "$\\frac{df(x)}{dx}=\\displaystyle\\lim_{h\\rarr\\infin}{\\frac{f(x+h)-f(x-h)}{2h}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "583d4d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f,x):\n",
    "    h=1e-4 # 0.0001\n",
    "    return(f(x+h)-f(x-h)/(2*h))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aad22af",
   "metadata": {},
   "source": [
    "### 편미분\n",
    "\n",
    "$f(x_0,x_1)\\;=\\;x^2_0+x^2_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dfabe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2+x[1]**2\n",
    "#또는 return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a69b35",
   "metadata": {},
   "source": [
    "문제: $x_0 = 3, x_1 = 4$ 일때, $x_0$ 에 대한 편미분 $\\frac{\\partial{f}}{\\partial{x_0}}$ 를 구하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4150b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-64983.99944998999"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 +4.0**2**0\n",
    "\n",
    "numerical_diff(function_tmp1,3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494a7f34",
   "metadata": {},
   "source": [
    "## 5. 기울기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49591aba",
   "metadata": {},
   "source": [
    "기울기가 가르키는 쪽은 각 장소에서 함수의 출력 값을 가장 크게 줄이는 방향"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7a29cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f,x):\n",
    "    h = 1e-4 #0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val #값 복원\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40843ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습률\n",
    "def gradient_descent(f,init_x,lr=0.01,step_num=100):\n",
    "    x = init_x\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f,x)\n",
    "        x -= lr*grad\n",
    "    return x\n",
    "# 인수 f는 최적화 하려는 함수, init_x는 초깃값, lr은 learning rate를 의미. step_num은 경사법에 따른 반복 횟수\n",
    "# 학습률을 곱한 값으로 갱신하는 처리를 step_num번 반복\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed7089",
   "metadata": {},
   "source": [
    "신경망에서도 기울기를 구해야함.   \n",
    "가중치 매개변수에 대한 손실함수에 대한 기울기.\n",
    "### simpleNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7b6de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpu as np\n",
    "import sys,os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__),'..'))\n",
    "from common.function import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2,3) #정규분포로 초기화\n",
    "        \n",
    "    def predict(self,x):\n",
    "        return np.dot(x,self.W)\n",
    "    def loss(self,x,t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y,t)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ae6830",
   "metadata": {},
   "source": [
    "common/functions.py에 정의한 softmax와 cross_entropy_error 매서드를 의용함   \n",
    "그리고 common.gradient.py에 정의한 numerical_gradient메서드도 이용.  \n",
    "\n",
    "simpleNet 클래스는 형상이 2x3인 2차원 배열    \n",
    "\n",
    "dW의 내용을 보면, $\\frac{\\partial L}{\\partial W}$ 의 $\\frac{\\partial L}{\\partial w_{11}}$ 은 대략 0.2 이다.  \n",
    "이는   $w_{11}$ 을 $h$ 만큼 늘리면 손실 함수의 값은 $0.2h$ 만큼 증가함을 의미.   \n",
    "\n",
    "마찬가지로 $\\frac{\\partial L}{\\partial w_{23}}$ 은 대략 -0.5이니,  $w_{23}$ 을 $h$ 만큼 늘리면 손실 함수의 값은 $0.5h$ 만큼 감소함을 의미.   \n",
    "\n",
    "$\\therefore w_{23} 이 w_{11} 보다 더 크게 기여함$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127af632",
   "metadata": {},
   "source": [
    "### 학습 알고리즘\n",
    "전체\n",
    "* 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향ㅇ르 훈련 데이터에 적응하도록 조정하는 과정을 학습이라함.   \n",
    "* 신경망 학습은 4단계로 수행\n",
    "1. 미니배치   \n",
    "* 훈련데이터 중 일부를 무작위로 가져옴.\n",
    "* 선별된 데이터를 미니배치라 하며, 미니배치의 손실 함수 값을 줄이는 거시 목표\n",
    "2. 기울기 산출\n",
    "* 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구함.\n",
    "* 기울기는 손실 함수의 값을 가장 작게 하는 방향\n",
    "3. 매개변수 갱신\n",
    "* 가중치 매개변수를 기울기 방향으로 아주 조금 갱신함.\n",
    "4. 반복\n",
    "* 1~3단계 반복\n",
    "\n",
    "이 때, 데이터를 미니배치로 무작위로 선정하기 때문에 경사하강법은 확률적 경사하강법 SGD 라고 부름   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a47f35",
   "metadata": {},
   "source": [
    "경사법은 기울기를 이용하는거임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbccb7",
   "metadata": {},
   "source": [
    "# 4장 신경망 학습 - 시험 대비 핵심 정리\n",
    "\n",
    "## 1. 학습의 개념 ⭐⭐⭐\n",
    "\n",
    "### 학습이란?\n",
    "\n",
    "**훈련 데이터로부터 가중치 매개변수의 최적값을 자동으로 찾는 과정**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "퍼셉트론: 가중치를 수동으로 설정\n",
    "신경망: 가중치를 데이터로부터 학습\n",
    "\n",
    "학습 = 손실 함수를 최소화하는 가중치 찾기\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### 손실 함수 (Loss Function)\n",
    "\n",
    "**신경망 모델의 예측 값과 실제 값 간의 차이를 측정하는 함수**\n",
    "\n",
    "- 신경망 성능의 \"나쁨\"을 나타내는 지표\n",
    "- 값이 작을수록 좋은 모델\n",
    "- 학습의 목표: 손실 함수를 최소화\n",
    "\n",
    "---\n",
    "\n",
    "## 2. 손실 함수의 종류 ⭐⭐⭐\n",
    "\n",
    "### 2.1 오차제곱합 (SSE: Sum of Squares for Error)\n",
    "\n",
    "**수식:**\n",
    "$$E = \\frac{1}{2}\\sum_{k}(y_k - t_k)^2$$\n",
    "\n",
    "- $y_k$: 신경망의 출력 (예측값)\n",
    "- $t_k$: 정답 레이블 (원-핫 인코딩)\n",
    "- $k$: 데이터의 차원 수\n",
    "\n",
    "**구현:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sum_squares_error(y, t):\n",
    "    \"\"\"오차제곱합\"\"\"\n",
    "    return 0.5 * np.sum((y - t)**2)\n",
    "\n",
    "# 예시\n",
    "# 정답: 2\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])  # 원-핫 인코딩\n",
    "\n",
    "# 예측 1: '2'일 확률이 가장 높음 (정답)\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(sum_squares_error(y1, t))  # 0.09750000000000003\n",
    "\n",
    "# 예측 2: '7'일 확률이 가장 높음 (오답)\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print(sum_squares_error(y2, t))  # 0.5975\n",
    "```\n",
    "\n",
    "**특징:**\n",
    "- 회귀 문제에 주로 사용\n",
    "- 오차의 제곱을 사용 (큰 오차에 더 큰 페널티)\n",
    "\n",
    "---\n",
    "\n",
    "### 2.2 교차 엔트로피 오차 (CEE: Cross Entropy Error) ⭐⭐⭐\n",
    "\n",
    "**수식:**\n",
    "$$E = -\\sum_{k}t_k\\log y_k$$\n",
    "\n",
    "- 정답일 때의 출력만 영향 (원-핫 인코딩)\n",
    "- 정답에 해당하는 출력이 클수록 손실 작음\n",
    "\n",
    "**구현:**\n",
    "```python\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"교차 엔트로피 오차\"\"\"\n",
    "    delta = 1e-7  # 아주 작은 값\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "\n",
    "# 예시\n",
    "t = np.array([0, 0, 1, 0, 0, 0, 0, 0, 0, 0])\n",
    "\n",
    "# 예측 1: '2'일 확률이 높음 (정답)\n",
    "y1 = np.array([0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0])\n",
    "print(cross_entropy_error(y1, t))  # 0.510825457099338\n",
    "\n",
    "# 예측 2: '7'일 확률이 높음 (오답)\n",
    "y2 = np.array([0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0])\n",
    "print(cross_entropy_error(y2, t))  # 2.302584092994546\n",
    "```\n",
    "\n",
    "**delta를 더하는 이유:**\n",
    "```python\n",
    "# np.log(0) = -∞ (마이너스 무한대)\n",
    "# 계산 불가능하므로 아주 작은 값을 더함\n",
    "\n",
    "y = 0\n",
    "print(np.log(y))        # 오류 또는 -inf\n",
    "print(np.log(y + 1e-7)) # -16.11809565095832\n",
    "```\n",
    "\n",
    "**특징:**\n",
    "- 분류 문제에 주로 사용\n",
    "- 정답 레이블에 해당하는 출력의 로그를 계산\n",
    "- 확률 분포 간의 차이를 측정\n",
    "\n",
    "---\n",
    "\n",
    "## 3. 미니배치 학습 ⭐⭐⭐\n",
    "\n",
    "### 배치용 손실 함수\n",
    "\n",
    "**수식:**\n",
    "$$E = -\\frac{1}{N}\\sum_{n}\\sum_{k}t_{nk}\\log y_{nk}$$\n",
    "\n",
    "- $N$: 배치 크기\n",
    "- 데이터 1개당 평균 손실 함수 계산\n",
    "\n",
    "### 미니배치란?\n",
    "\n",
    "**훈련 데이터 중 일부를 무작위로 선택하여 학습하는 방법**\n",
    "\n",
    "```python\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "import numpy as np\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(\n",
    "    normalize=True, \n",
    "    one_hot_label=True\n",
    ")\n",
    "\n",
    "print(x_train.shape)  # (60000, 784)\n",
    "print(t_train.shape)  # (60000, 10)\n",
    "\n",
    "# 미니배치 선택\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]\n",
    "\n",
    "print(x_batch.shape)  # (10, 784)\n",
    "print(t_batch.shape)  # (10, 10)\n",
    "```\n",
    "\n",
    "### 배치용 교차 엔트로피 구현\n",
    "\n",
    "```python\n",
    "def cross_entropy_error(y, t):\n",
    "    \"\"\"\n",
    "    배치 처리를 지원하는 교차 엔트로피\n",
    "    \n",
    "    y: 신경망 출력\n",
    "    t: 정답 레이블\n",
    "    \"\"\"\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "    \n",
    "    # 배치 크기\n",
    "    batch_size = y.shape[0]\n",
    "    \n",
    "    # 원-핫 인코딩인 경우\n",
    "    if t.size == y.size:\n",
    "        return -np.sum(t * np.log(y + 1e-7)) / batch_size\n",
    "    \n",
    "    # 정답 레이블이 숫자인 경우\n",
    "    else:\n",
    "        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "```\n",
    "\n",
    "**사용 예:**\n",
    "```python\n",
    "# 원-핫 인코딩\n",
    "t = np.array([[0, 0, 1, 0],\n",
    "              [0, 1, 0, 0]])\n",
    "y = np.array([[0.1, 0.2, 0.6, 0.1],\n",
    "              [0.2, 0.5, 0.2, 0.1]])\n",
    "print(cross_entropy_error(y, t))\n",
    "\n",
    "# 레이블 형식\n",
    "t = np.array([2, 1])  # 정답 인덱스\n",
    "y = np.array([[0.1, 0.2, 0.6, 0.1],\n",
    "              [0.2, 0.5, 0.2, 0.1]])\n",
    "print(cross_entropy_error(y, t))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 4. 왜 손실 함수를 사용하는가? ⭐⭐⭐\n",
    "\n",
    "### 정확도를 지표로 사용하면 안 되는 이유\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "정확도 문제점:\n",
    "1. 불연속적 (0% → 100%)\n",
    "2. 미분이 대부분 0\n",
    "3. 매개변수의 미세한 변화를 반영 못함\n",
    "\n",
    "예:\n",
    "가중치를 조금 변경 → 정확도 변화 없음 (33% → 33%)\n",
    "→ 기울기가 0 → 학습 불가능\n",
    "\n",
    "손실 함수:\n",
    "1. 연속적\n",
    "2. 미분 가능\n",
    "3. 매개변수의 미세한 변화 반영\n",
    "→ 학습 가능!\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**예시:**\n",
    "```python\n",
    "# 100개 데이터 중 32개 맞춤 → 정확도 32%\n",
    "# 가중치 조금 변경\n",
    "# 100개 데이터 중 32개 맞춤 → 정확도 32% (변화 없음!)\n",
    "\n",
    "# 하지만 손실 함수는:\n",
    "# 손실: 2.3\n",
    "# 가중치 조금 변경\n",
    "# 손실: 2.29 (감소!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5. 수치 미분 ⭐⭐\n",
    "\n",
    "### 미분의 정의\n",
    "\n",
    "**해석적 미분:**\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x)}{h}$$\n",
    "\n",
    "**문제점:** 반올림 오차 (rounding error)\n",
    "\n",
    "### 수치 미분 (Numerical Differentiation)\n",
    "\n",
    "**중심 차분 / 중앙 차분:**\n",
    "$$\\frac{df(x)}{dx} = \\lim_{h \\to 0}\\frac{f(x+h) - f(x-h)}{2h}$$\n",
    "\n",
    "**구현:**\n",
    "```python\n",
    "def numerical_diff(f, x):\n",
    "    \"\"\"수치 미분\"\"\"\n",
    "    h = 1e-4  # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "\n",
    "# 예시: f(x) = x^2\n",
    "def function_1(x):\n",
    "    return x**2\n",
    "\n",
    "# x=5에서의 미분\n",
    "print(numerical_diff(function_1, 5))   # 10.000000000139778\n",
    "print(numerical_diff(function_1, 10))  # 20.00000000279556\n",
    "\n",
    "# 해석적 미분: f'(x) = 2x\n",
    "# x=5 → 10\n",
    "# x=10 → 20\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 6. 편미분 ⭐⭐\n",
    "\n",
    "### 다변수 함수의 미분\n",
    "\n",
    "**함수:**\n",
    "$$f(x_0, x_1) = x_0^2 + x_1^2$$\n",
    "\n",
    "**구현:**\n",
    "```python\n",
    "def function_2(x):\n",
    "    \"\"\"f(x0, x1) = x0^2 + x1^2\"\"\"\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 또는 return np.sum(x**2)\n",
    "```\n",
    "\n",
    "### 편미분 계산\n",
    "\n",
    "**문제:** $x_0 = 3, x_1 = 4$일 때, $\\frac{\\partial f}{\\partial x_0}$를 구하라.\n",
    "\n",
    "```python\n",
    "# x1을 4로 고정\n",
    "def function_tmp1(x0):\n",
    "    return x0**2 + 4.0**2\n",
    "\n",
    "print(numerical_diff(function_tmp1, 3.0))  # 6.00000000000378\n",
    "\n",
    "# 해석적 미분: ∂f/∂x0 = 2x0\n",
    "# x0=3 → 2*3 = 6\n",
    "```\n",
    "\n",
    "**문제:** $x_0 = 3, x_1 = 4$일 때, $\\frac{\\partial f}{\\partial x_1}$를 구하라.\n",
    "\n",
    "```python\n",
    "# x0을 3으로 고정\n",
    "def function_tmp2(x1):\n",
    "    return 3.0**2 + x1**2\n",
    "\n",
    "print(numerical_diff(function_tmp2, 4.0))  # 7.999999999999119\n",
    "\n",
    "# 해석적 미분: ∂f/∂x1 = 2x1\n",
    "# x1=4 → 2*4 = 8\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 7. 기울기 (Gradient) ⭐⭐⭐\n",
    "\n",
    "### 기울기란?\n",
    "\n",
    "**모든 변수의 편미분을 벡터로 정리한 것**\n",
    "\n",
    "$$\\nabla f = \\left(\\frac{\\partial f}{\\partial x_0}, \\frac{\\partial f}{\\partial x_1}\\right)$$\n",
    "\n",
    "### 구현\n",
    "\n",
    "```python\n",
    "def numerical_gradient(f, x):\n",
    "    \"\"\"기울기 계산\"\"\"\n",
    "    h = 1e-4  # 0.0001\n",
    "    grad = np.zeros_like(x)  # x와 같은 형상의 배열\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        \n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        \n",
    "        # 중심 차분\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        # 값 복원\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# 테스트\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# (3, 4)에서의 기울기\n",
    "print(numerical_gradient(function_2, np.array([3.0, 4.0])))\n",
    "# [6. 8.]\n",
    "\n",
    "# (0, 2)에서의 기울기\n",
    "print(numerical_gradient(function_2, np.array([0.0, 2.0])))\n",
    "# [0. 4.]\n",
    "\n",
    "# (3, 0)에서의 기울기\n",
    "print(numerical_gradient(function_2, np.array([3.0, 0.0])))\n",
    "# [6. 0.]\n",
    "```\n",
    "\n",
    "### 기울기의 의미\n",
    "\n",
    "**기울기가 가리키는 방향 = 각 지점에서 함수의 출력 값을 가장 크게 줄이는 방향**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "f(x0, x1) = x0^2 + x1^2\n",
    "\n",
    "(3, 4)에서 기울기: [6, 8]\n",
    "→ x0를 음의 방향으로, x1을 음의 방향으로 이동하면\n",
    "  함수 값이 감소\n",
    "\n",
    "원점 (0, 0)에서 기울기: [0, 0]\n",
    "→ 최솟값 지점 (극솟값)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 8. 경사하강법 (Gradient Descent) ⭐⭐⭐\n",
    "\n",
    "### 개념\n",
    "\n",
    "**기울기를 이용하여 함수의 최솟값을 찾는 방법**\n",
    "\n",
    "$$x_{new} = x_{old} - \\eta \\nabla f(x_{old})$$\n",
    "\n",
    "- $\\eta$: 학습률 (learning rate)\n",
    "- $\\nabla f$: 기울기\n",
    "\n",
    "### 구현\n",
    "\n",
    "```python\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    \"\"\"\n",
    "    경사하강법\n",
    "    \n",
    "    f: 최적화할 함수\n",
    "    init_x: 초깃값\n",
    "    lr: 학습률 (learning rate)\n",
    "    step_num: 반복 횟수\n",
    "    \"\"\"\n",
    "    x = init_x\n",
    "    x_history = []  # 이동 경로 기록\n",
    "    \n",
    "    for i in range(step_num):\n",
    "        x_history.append(x.copy())\n",
    "        \n",
    "        # 기울기 계산\n",
    "        grad = numerical_gradient(f, x)\n",
    "        \n",
    "        # 갱신\n",
    "        x -= lr * grad\n",
    "    \n",
    "    return x, np.array(x_history)\n",
    "\n",
    "# 테스트\n",
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "\n",
    "# 초깃값 (-3, 4)\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "\n",
    "# 학습률 0.1\n",
    "x, history = gradient_descent(function_2, init_x, lr=0.1, step_num=100)\n",
    "print(x)  # [-6.11110793e-10  8.14814391e-10] ≈ (0, 0)\n",
    "```\n",
    "\n",
    "### 학습률의 중요성 ⭐⭐\n",
    "\n",
    "```python\n",
    "# 학습률이 너무 큰 경우\n",
    "x, _ = gradient_descent(function_2, init_x, lr=10.0, step_num=100)\n",
    "print(x)  # [-2.58983747e+13 -1.29524862e+12] → 발산!\n",
    "\n",
    "# 학습률이 너무 작은 경우\n",
    "x, _ = gradient_descent(function_2, init_x, lr=1e-10, step_num=100)\n",
    "print(x)  # [-2.99999994  3.99999992] → 거의 이동 안 함!\n",
    "\n",
    "# 적절한 학습률\n",
    "x, _ = gradient_descent(function_2, init_x, lr=0.1, step_num=100)\n",
    "print(x)  # [0. 0.] → 최솟값 도달!\n",
    "```\n",
    "\n",
    "**핵심:**\n",
    "- 학습률이 너무 크면 → 발산\n",
    "- 학습률이 너무 작으면 → 학습 안 됨\n",
    "- 적절한 학습률 선택이 중요 (하이퍼파라미터)\n",
    "\n",
    "---\n",
    "\n",
    "## 9. 신경망의 기울기 ⭐⭐⭐\n",
    "\n",
    "### SimpleNet 구현\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import sys, os\n",
    "sys.path.append(os.path.join(os.path.dirname(__file__), '..'))\n",
    "from common.functions import softmax, cross_entropy_error\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class SimpleNet:\n",
    "    def __init__(self):\n",
    "        # 가중치 초기화 (2×3 행렬)\n",
    "        self.W = np.random.randn(2, 3)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"예측\"\"\"\n",
    "        return np.dot(x, self.W)\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 계산\"\"\"\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "        return loss\n",
    "\n",
    "# 사용 예시\n",
    "net = SimpleNet()\n",
    "print(net.W)\n",
    "# [[ 0.47355232  0.9977393   0.84668094]\n",
    "#  [ 0.85557411  0.03563661  0.69422093]]\n",
    "\n",
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)\n",
    "# [1.05414809 0.63071653 1.13269074]\n",
    "\n",
    "print(np.argmax(p))  # 2 (최댓값 인덱스)\n",
    "\n",
    "t = np.array([0, 0, 1])  # 정답 레이블\n",
    "print(net.loss(x, t))    # 0.92806853663411326\n",
    "```\n",
    "\n",
    "### 가중치에 대한 기울기\n",
    "\n",
    "```python\n",
    "def f(W):\n",
    "    \"\"\"손실 함수 (가중치를 인수로)\"\"\"\n",
    "    return net.loss(x, t)\n",
    "\n",
    "# 기울기 계산\n",
    "dW = numerical_gradient(f, net.W)\n",
    "print(dW)\n",
    "# [[ 0.21924763  0.14356247 -0.36281009]\n",
    "#  [ 0.32887144  0.2153437  -0.54421514]]\n",
    "```\n",
    "\n",
    "### 기울기의 의미\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "dW[0, 0] ≈ 0.22\n",
    "→ w11을 h만큼 증가시키면 손실이 0.22h만큼 증가\n",
    "\n",
    "dW[1, 2] ≈ -0.54\n",
    "→ w23을 h만큼 증가시키면 손실이 0.54h만큼 감소\n",
    "\n",
    "따라서:\n",
    "- 양수 기울기: 가중치를 감소시켜야 함\n",
    "- 음수 기울기: 가중치를 증가시켜야 함\n",
    "- 절댓값이 클수록: 손실에 더 큰 영향\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10. 학습 알고리즘 구현 ⭐⭐⭐\n",
    "\n",
    "### 신경망 학습 4단계\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "1단계: 미니배치\n",
    "   - 훈련 데이터 중 일부를 무작위로 선택\n",
    "\n",
    "2단계: 기울기 산출\n",
    "   - 각 가중치 매개변수에 대한 손실 함수의 기울기 계산\n",
    "\n",
    "3단계: 매개변수 갱신\n",
    "   - 가중치를 기울기 방향으로 조금 갱신\n",
    "\n",
    "4단계: 반복\n",
    "   - 1~3단계를 반복\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### 2층 신경망 클래스\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from common.functions import *\n",
    "from common.gradient import numerical_gradient\n",
    "\n",
    "class TwoLayerNet:\n",
    "    def __init__(self, input_size, hidden_size, output_size, \n",
    "                 weight_init_std=0.01):\n",
    "        \"\"\"\n",
    "        초기화\n",
    "        \n",
    "        input_size: 입력층 뉴런 수\n",
    "        hidden_size: 은닉층 뉴런 수\n",
    "        output_size: 출력층 뉴런 수\n",
    "        \"\"\"\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * \\\n",
    "                           np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * \\\n",
    "                           np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"예측 (순전파)\"\"\"\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        \"\"\"손실 함수 값\"\"\"\n",
    "        y = self.predict(x)\n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        \"\"\"정확도\"\"\"\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "    \n",
    "    def numerical_gradient(self, x, t):\n",
    "        \"\"\"기울기 계산 (수치 미분)\"\"\"\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "```\n",
    "\n",
    "### 학습 구현\n",
    "\n",
    "```python\n",
    "from dataset.mnist import load_mnist\n",
    "\n",
    "# 데이터 로드\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(\n",
    "    normalize=True, \n",
    "    one_hot_label=True\n",
    ")\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000  # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100\n",
    "learning_rate = 0.1\n",
    "\n",
    "train_loss_list = []\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "\n",
    "# 1에폭당 반복 수\n",
    "iter_per_epoch = max(train_size / batch_size, 1)\n",
    "\n",
    "# 신경망 생성\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "    \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)\n",
    "    \n",
    "    # 1에폭당 정확도 계산\n",
    "    if i % iter_per_epoch == 0:\n",
    "        train_acc = network.accuracy(x_train, x_test)\n",
    "        test_acc = network.accuracy(x_test, t_test)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "        print(f\"train acc, test acc | {train_acc}, {test_acc}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 11. 용어 정리 ⭐⭐⭐\n",
    "\n",
    "### 에폭 (Epoch)\n",
    "\n",
    "**모든 훈련 데이터를 한 번씩 학습한 것**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "훈련 데이터: 60,000개\n",
    "미니배치: 100개\n",
    "\n",
    "1 에폭 = 60,000 / 100 = 600회 반복\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "### 확률적 경사하강법 (SGD: Stochastic Gradient Descent)\n",
    "\n",
    "**무작위로 선택한 미니배치에 대해 경사하강법을 수행**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "전체 데이터: 비용이 너무 큼\n",
    "미니배치: 무작위 선택 → 확률적 (Stochastic)\n",
    "\n",
    "장점:\n",
    "- 빠른 학습\n",
    "- 메모리 효율적\n",
    "- 지역 최솟값 탈출 가능\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 12. 시험 예상 문제\n",
    "\n",
    "### 문제 1 (10점)\n",
    "```\n",
    "오차제곱합(SSE)과 교차 엔트로피 오차(CEE)의 수식을 쓰고,\n",
    "각각 어떤 문제에 주로 사용되는지 설명하시오.\n",
    "```\n",
    "\n",
    "**답안:**\n",
    "```\n",
    "1. 오차제곱합 (SSE):\n",
    "   E = (1/2)Σ(yk - tk)²\n",
    "   - 회귀 문제에 주로 사용\n",
    "   - 예측값과 실제값의 차이를 제곱하여 합산\n",
    "\n",
    "2. 교차 엔트로피 오차 (CEE):\n",
    "   E = -Σtk log(yk)\n",
    "   - 분류 문제에 주로 사용\n",
    "   - 정답 레이블에 해당하는 출력의 로그를 계산\n",
    "   - 확률 분포 간의 차이를 측정\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 문제 2 (10점)\n",
    "```\n",
    "신경망 학습에서 정확도가 아닌 손실 함수를 사용하는 \n",
    "이유를 설명하시오.\n",
    "```\n",
    "\n",
    "**답안:**\n",
    "```\n",
    "정확도는 불연속적이고 대부분의 지점에서 미분값이 0이 되어\n",
    "매개변수의 미세한 변화를 반영하지 못하기 때문이다.\n",
    "\n",
    "예를 들어, 100개 데이터 중 32개를 맞추면 정확도는 32%이고,\n",
    "가중치를 조금 변경해도 여전히 32개를 맞추면 정확도는 32%로\n",
    "변화가 없다. 이 경우 기울기가 0이 되어 학습이 불가능하다.\n",
    "\n",
    "반면 손실 함수는 연속적이고 미분 가능하여 매개변수의\n",
    "미세한 변화도 반영할 수 있으므로 학습이 가능하다.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 문제 3 (15점)\n",
    "```\n",
    "다음 함수에 대해 (3, 4) 지점에서의 기울기를 구하시오.\n",
    "\n",
    "f(x0, x1) = x0² + x1²\n",
    "\n",
    "그리고 기울기의 의미를 설명하시오.\n",
    "```\n",
    "\n",
    "**답안:**\n",
    "```python\n",
    "# 편미분\n",
    "∂f/∂x0 = 2x0 = 2(3) = 6\n",
    "∂f/∂x1 = 2x1 = 2(4) = 8\n",
    "\n",
    "# 기울기\n",
    "∇f = (6, 8)\n",
    "\n",
    "# 의미:\n",
    "기울기 (6, 8)은 (3, 4) 지점에서 함수 f의 값이\n",
    "가장 가파르게 증가하는 방향을 나타낸다.\n",
    "\n",
    "따라서 함수의 최솟값을 찾으려면 기울기의 반대 방향인\n",
    "(-6, -8) 방향으로 이동해야 한다.\n",
    "\n",
    "이것이 경사하강법의 원리이다.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 문제 4 (15점)\n",
    "```\n",
    "경사하강법의 갱신 수식을 쓰고, 학습률이 너무 크거나\n",
    "너무 작을 때 발생하는 문제를 설명하시오.\n",
    "```\n",
    "\n",
    "**답안:**\n",
    "```\n",
    "갱신 수식:\n",
    "x_new = x_old - η∇f(x_old)\n",
    "\n",
    "여기서 η는 학습률(learning rate)\n",
    "\n",
    "학습률이 너무 큰 경우:\n",
    "- 갱신 폭이 너무 커서 최솟값을 지나쳐 버림\n",
    "- 발산(divergence)할 수 있음\n",
    "- 예: η = 10.0 → 값이 무한대로 증가\n",
    "\n",
    "학습률이 너무 작은 경우:\n",
    "- 갱신 폭이 너무 작아서 학습이 느림\n",
    "- 최솟값에 도달하지 못함\n",
    "- 예: η = 1e-10 → 거의 이동하지 않음\n",
    "\n",
    "따라서 적절한 학습률을 선택하는 것이 중요하다.\n",
    "(일반적으로 0.01, 0.001 등 사용)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 문제 5 (10점)\n",
    "```\n",
    "미니배치 학습의 장점 3가지를 설명하시오.\n",
    "```\n",
    "\n",
    "**답안:**\n",
    "```\n",
    "1. 계산 효율성\n",
    "   - 전체 데이터를 한 번에 처리하는 것보다 빠름\n",
    "   - 행렬 연산의 최적화 활용\n",
    "\n",
    "2. 메모리 효율성\n",
    "   - 전체 데이터를 메모리에 올릴 필요 없음\n",
    "   - GPU 메모리 제약 극복\n",
    "\n",
    "3. 일반화 성능 향상\n",
    "   - 무작위 선택으로 인한 노이즈가 \n",
    "     지역 최솟값 탈출에 도움\n",
    "   - 과적합(overfitting) 방지\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 13. 핵심 개념 정리\n",
    "\n",
    "### ✅ 반드시 알아야 할 것\n",
    "\n",
    "1. **손실 함수**\n",
    "   - SSE: 회귀 문제\n",
    "   - CEE: 분류 문제\n",
    "   - 원-핫 인코딩\n",
    "\n",
    "2. **미니배치**\n",
    "   - 일부 데이터로 학습\n",
    "   - 평균 손실 계산\n",
    "   - 확률적 경사하강법\n",
    "\n",
    "3. **수치 미분**\n",
    "   - 중심 차분 사용\n",
    "   - h = 1e-4\n",
    "   - 근사값 계산\n",
    "\n",
    "4. **기울기**\n",
    "   - 모든 변수의 편미분\n",
    "   - 함수가 가장 크게 감소하는 방향\n",
    "   - 경사하강법에 사용\n",
    "\n",
    "5. **경사하강법**\n",
    "   - x_new = x_old - η∇f\n",
    "   - 학습률의 중요성\n",
    "   - 반복적 갱신\n",
    "\n",
    "6. **학습 알고리즘**\n",
    "   - 미니배치 → 기울기 → 갱신 → 반복\n",
    "   - 에폭: 전체 데이터 1회 학습\n",
    "   - SGD: 확률적 경사하강법\n",
    "\n",
    "---\n",
    "\n",
    "## 14. 빠른 복습 체크리스트\n",
    "\n",
    "- [ ] 손실 함수의 개념과 종류\n",
    "- [ ] SSE와 CEE 수식\n",
    "- [ ] 미니배치 학습\n",
    "- [ ] 정확도 대신 손실 함수를 쓰는 이유\n",
    "- [ ] 수치 미분 (중심 차분)\n",
    "- [ ] 편미분의 개념\n",
    "- [ ] 기울기의 의미\n",
    "- [ ] 경사하강법 수식\n",
    "- [ ] 학습률의 역할\n",
    "- [ ] 신경망 학습 4단계\n",
    "- [ ] 에폭과 SGD 개념\n",
    "\n",
    "---\n",
    "\n",
    "## 15. 코드 템플릿 (암기용)\n",
    "\n",
    "### 손실 함수\n",
    "\n",
    "```python\n",
    "# SSE\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y - t)**2)\n",
    "\n",
    "# CEE\n",
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y + delta))\n",
    "```\n",
    "\n",
    "### 수치 미분\n",
    "\n",
    "```python\n",
    "def numerical_diff(f, x):\n",
    "    h = 1e-4\n",
    "    return (f(x+h) - f(x-h)) / (2*h)\n",
    "```\n",
    "\n",
    "### 기울기\n",
    "\n",
    "```python\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val\n",
    "    \n",
    "    return grad\n",
    "```\n",
    "\n",
    "### 경사하강법\n",
    "\n",
    "```python\n",
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "    return x\n",
    "```\n",
    "\n",
    "**시험 화이팅! 🎯**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e777564c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

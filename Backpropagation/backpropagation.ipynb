{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefd70e8",
   "metadata": {},
   "source": [
    "# Backpropagtion\n",
    "\n",
    "## 단순계층 구현하기\n",
    "모든계층은 forward()와 backward()라는 공통의 메서드(인터페이스)를  갑ㅈ도록 구현할 것.\n",
    "forward()는 순전파, backward()은 역전파."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251f5de5",
   "metadata": {},
   "source": [
    "### 1. 곱셈계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48c0bba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MlLayer:\n",
    "    def __init__(self):\n",
    "        self.x = None\n",
    "        self.y = None\n",
    "    def forward(self, x,y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        out = x*y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = dout*self.y\n",
    "        dy = dout*self.x\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50d27d7f",
   "metadata": {},
   "source": [
    "### 2. 덧셈계층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e201da35",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddLayer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def forward(self,x,y):\n",
    "        out = x+y\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = dout *1\n",
    "        dy = dout*1\n",
    "        return dx,dy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7918ac",
   "metadata": {},
   "source": [
    "## 활성화 함수 계층 구현하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e248b3f7",
   "metadata": {},
   "source": [
    "### 1. ReLU 계층\n",
    "\n",
    "$$y = \\begin{cases}{ x\\;(\\;>\\;0)}\\\\ {0\\;(\\leq\\;0)}\\end{cases}$$\n",
    "\n",
    "$${\\frac{\\partial y}{\\partial x}} = \\begin{cases} {1\\;(x>\\;0)}\\\\{0\\;(\\leq\\;0)}\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c81b909d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    def __init__(self):\n",
    "        self.mask=None\n",
    "    def forward(self,x):\n",
    "        self.mask = (x<=0)\n",
    "        out = x.copy()\n",
    "        out[self.mask]=0\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dout[self.mask]=0\n",
    "        dx =dout\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b04671f",
   "metadata": {},
   "source": [
    "### 2. Sigmoid 계층\n",
    "\n",
    "$$y = \\frac{1}{1+exp(-x)} $$\n",
    "\n",
    "$$ \\frac{\\partial y}{\\partial x} = -\\frac{1}{x^2} = -y^2 $$\n",
    "  \n",
    "$$\\frac{\\partial exp(x)}{\\partial x} = exp(x)$$\n",
    "\n",
    "$$\\therefore \\;\\frac{\\partial L}{\\partial y}y^2exp(-x)\\;=\\; \\frac{\\partial L}{\\partial y}y(1-y)$$\n",
    "\n",
    "이처럼 Sigmoid 계층의 역전파는 순전파의 출력 y만으로 계산할 수 잇음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d650c3c",
   "metadata": {},
   "source": [
    "순전파의 출력을 인스턴스 변수 out에 보관했다가 역전파 때 사용하는걸 알 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e055b371",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid:\n",
    "    def __init__(self):\n",
    "        self.out = None\n",
    "    def forward(self,x):\n",
    "        out = 1/(1+np.exp(-x))\n",
    "        self.out = out\n",
    "        return out\n",
    "    def backward(self,dout):\n",
    "        dx = dout *(1.0 - self.out)*self.out\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f0516",
   "metadata": {},
   "source": [
    "## 3. Affine/Softmax 계층 구현하기\n",
    "\n",
    "행렬곱은 기하학에서 Affine transformation이라고함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7288a48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "class Affine:\n",
    "    def __init__(self,W,b):\n",
    "        self.W=W\n",
    "        self.b=b\n",
    "        self.x =None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "    def forward(self,x):\n",
    "        self.x = x\n",
    "        out = np.dot(x, self.W)+ self.b\n",
    "        return out\n",
    "    \n",
    "    def backward(self,dout):\n",
    "        dx = np.dot(dout,self.W.T)\n",
    "        self.dW = np.dot(self.x.T,dout)\n",
    "        self.db = np.sum(dout, axis=0)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2200f948",
   "metadata": {},
   "source": [
    "### 4. Softmax-with-Loss 계층\n",
    "\n",
    "softmax는 분류문제 (classification problem)에서 사용. 소프트맥스함수의 손실함수로 교차 엔트로피 오차를 사용하니, 역전파가 y_1 - t_1로 깔끔히 떨어짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb44368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWithLoss:\n",
    "    def __init__(self):\n",
    "        self.loss = None #손실함수\n",
    "        self.y = None #softmax의 출력\n",
    "        self.t = None #정답 레이블(원-핫 인코딩)\n",
    "    def forward(self,,x,t):\n",
    "        self.t=tself.y=softmax(x)\n",
    "        self.loss = cross_entropy_error(self.y,self.t)\n",
    "        return self.loss\n",
    "    def backward(self,dout=1):\n",
    "        batch_size=self.t.shape[0]\n",
    "        dx = (sself.y-self.t)/batch_size\n",
    "        return dx"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
